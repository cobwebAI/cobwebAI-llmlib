Мы сегодня с вами говорим про нейронные сети, наконец-то переходим. Начнем с самых простых объектов архитектуры нейронных сетей, то есть это модель перцептрона. Из перцептронов мы будем делать с вами многослойные нейронные сети. Мы с вами говорили про постановку задач машинного обучения с учителем. Я быстренько напомню, что у нас есть данные x и ответ к ним y. Это пример с квартирами, что у нас каждый врага это некий объект квартиры, у каждой квартиры есть некие признаки и есть ответы, это цена. И, допустим, нам нужно предсказать цену. То есть у нас есть x, у каждого x при этом есть признаки x1, x2, x3 и так далее, и есть y. И, соответственно, задачи машинного обучения строятся вот таким образом. Нам нужно подобрать алгоритм A таким образом, чтобы получать отображение из x к y. При этом мы понимаем, что невозможно на всех данных сделать абсолютно точное отображение, поэтому мы вводим loss функцию, которая нам показывает, насколько близко мы приближаемся к ответам. То есть это функция ошибки. И задача сводится к тому, что нам нужно минимизировать эту функцию ошибки. И делаем это мы, меняя вот эти параметры модели. Сейчас я посмотрю, видно. Мы с вами успели поговорить про линейные модели. Опять же, кратко повторюсь, что формула для линейной модели выглядит таким образом. То есть это матричная запись. Я напоминаю, что мы пользуемся матричными операциями, и вы сегодня увидите, почему именно ими. Получается, что loss функция для задачи регрессии – это MSE и Maya. Мы с вами говорили, как они выглядят, что они делают, и как это делается. Мы делаем градиентный спуск. Чтобы посчитать градиентный спуск, нам нужно посчитать градиент. И мы обсуждали, что линейный регрессион выглядит вот таким образом. Правда, транспонирование у нас было вот здесь. Но это зависит от того, как вы задаете матрицы x, y и так далее. Это не важно, просто зависит от того, как вы задаете матрицы. Но в целом формула выглядит вот таким образом. И говорили с вами про задачу классификации. В частности, бинарной классификации. И говорили, что в классификации немножко по-другому выглядит задача. Нам нужно вот эту прямую выделить, которая разделяет объект на два класса. И чтобы это сделать, мы вводим такую функцию sigmoid. На вход этой функции подаем линейную комбинацию. И предсказаниями тогда y будет вот такая функция. То есть на вход этой функции подается линейная комбинация x на y. То есть у нас появляется промежуточная переменная z, которая является линейной комбинацией. И классфункция для логистической регрессии выглядит вот таким образом. Здесь у нас по благорифмуме вот эти самые предсказания. При этом кто-то мне напомнит, какие значения эти предсказания принимают в таком виде, которые после sigmoid. Сигмойд выглядит таким образом. Она стремится к нулю, стремится к единичке. И получается, что от нуля до единицы. И поэтому вот здесь будут значения от нуля до единицы. Здесь один минус. А вот здесь стоят либо ноль, либо один. Соответственно, если ноль, то вот это обнуляется. И остается вот это. Если y равен 1, то вот это обнуляется. И остается только вот это. Во всех классфункциях я напомню, что мы усредняем по всем объектам из датасета. То есть везде стоит сумма по всем объектам из датасета. Это то, что было в прошлый раз. И мы отталкивались от этого. Теперь будем строить с вами нейронные сети. Давайте сначала я вам расскажу, откуда вообще растут ноги. Почему нейронные сети стали вот так популярны. На самом деле вы увидите, что первые модели нейронных сетей были еще в 20 веке. Но они были в основном теоретические. То есть не было достаточно железа, вычислителей. Чтобы нейронные сети обучать, нужны данные и вычислители, чтобы их обучать. Нужно где-то хранить эти данные и их обучать. Поэтому даже в 90-х, когда компьютеры уже развивались и стали напоминать то, что мы сейчас видим, все равно была проблема именно с хранением данных. Вы помните эти дискеты, которые несколько мегабайт можно было хранить. Это еще в 90-х было. Я застал в это время. А вот уже начиная с нулевых, это стало развиваться и появились видеокарточки. Вот тут как раз там, где искусственный интеллект нашел хайп, это обучение нейронных сетей для распознавания образов на изображении. Это сетка AlexNet. Эта сетка, в чем прикол. Раньше применяли классические алгоритмы компьютерного зрения, чтобы решать эти задачи. А тут взяли просто нейронку, обучили, и она кратно превосходила по метрикам классические подходы. И вот тут люди поняли, что когда у вас есть много данных, чем больше вы подаете данных, тем у вас качество нейронных сетей все больше и больше растет. А у классических алгоритмов машинного обучения, которые вы будете проходить в следующем семестре, их качество выходит на некое плато. И, соответственно, получается, мы даже сейчас это видим, то есть какой-нибудь OpenAI берет еще жирнее модели, еще больше данных, и у них качество все растет и растет. Так они уже весь интернет продаются. Ну, короче, они продолжают этот подход. Он заморгал. А что с ним, он перегревается просто? Понятно. Но пока вас не было, только видели. Блин, ну что делать? Я могу, в принципе, просто развернуть монитор. Давайте я договорю мысли. В общем, суть в том, что на нейронке они в этом плане, у них меньше ограничений по качеству, вы на них можете подавать больше данных, и их качество будет все лучше и лучше. И, собственно, когда в нулевые годы появились видеокарточки, и стало возможным хранить много данных, тут вот нейронки хайпанули, и весь этот хайп пошел, по сути, начиная с нулевых 2010-го, По-моему, в прошлом году было 25 лет, когда они появились. Ну вот, это сейчас, скажу, получается какой год-то. 1998-го, да. Знаете что, я вам даже покажу видос. Сейчас, подойдите, перейдите. То есть нейронки, это явно DeepLearn. Еще раз вопрос. То есть сами нейронки, это именно DeepLearn, или еще какие-то? Ну, в основном, да, я имею в виду DeepLearn. То есть, стоит посоветовать машин, которые нейронки? Ну, потому что, давайте так. В принципе, алгоритмы линейной регрессии, они, известно, очень давно и давно использовались. На самом деле, весь этот хайп про Data Science и так далее, я приводил даже личный пример. Я, по-моему, рассказывал, что я когда учился в университете, у меня магистрская диссертация была посвящена задачке, которая связана была с обработкой данных на большом адронном коллайдере. И там повсеместно применяются эти статистические подходы для обработки данных. И оказывается, это Data Science и Machine Learning. Но эти подходы, мне кажется, были, грубо говоря, известны давно, и их применяли гораздо раньше, чем нейронки. Нейронки стало возможным применять именно из-за того, что возник возможность их обучать и хранить много данных. Данные стало возможным тоже хранить чуть раньше, а именно применять видеокарты, чтобы их обучать, стало возможным относительно недавно. Поэтому, собственно, они вот так стрельнули в последнее время, и это все только набирает обороты. Собственно, тут я сразу скажу, у меня слайды немножко недоделаны, потому что мы в процессе того, что делаем курс. Но я буду иногда переключаться сюда. Кстати говоря, вот эти материалы, это курс на Курсере, но можно на Ютубе найти видеозаписи выложенного. Я очень много подчеркнул отсюда. Так вот, в чем проблема линейных моделей, кто мне скажет? Предсказание линейной зависимости. Когда у вас возникает нелинейная зависимость, у вас линейная модель не очень хорошо работает. Вот тут пример, например, нелинейность. Тут зависимость цены от размера, и видно, что здесь 0,0,0,0, а потом с какого-то момента начинает расти. Тут возникает нелинейность, и получается, что ваша линейная модель, наверное, можно построить, но вот здесь возникает нелинейность. Здесь приводится пример, что у вас есть разные фичи, и вы их можете подавать независимо друг от друга на кодек, и они дальше могут идти в такие штуки. Но это мы сейчас отдельно с вами поговорим, поэтому сейчас не буду на этом останавливаться. Про обучение с учителем я говорил, то есть с какими данными можно применять нейронные сети. То есть это табличные данные, аудио, картинки, тексты. Это вы и так знаете, поэтому это мы быстро пропустим. Вот та самая картинка, которую я показывал. Кто не видит, здесь традиционного алгоритма, здесь какие-то нейронные сети, а и тут размеры нейронных сетей. Это маленькие, средние, большие архитектуры, и видно, как у них меняется качество в зависимости от количества данных, которые вы туда подаете. Собственно говоря, здесь показано, как выглядит работа в Deep Learning. То есть у вас появляется идея, вы ее забиваете в код, проводите эксперимент, получается обратную связь, и у вас возникает на основе этого новая идея. То есть это довольно практичная область, во многом экспериментальная. Давайте перейдем к логистической регрессии.
Все, запись начала, отлично, теперь точно можно начинать. Так вот, быстро повторюсь, значит, мы сегодня с вами говорим про нейронные сети, наконец-то переходим, да? Вот, начнем с самых простых объектов архитектуры нейронных сетей, то есть это модель перцептрона. Из перцептронов мы будем делать с вами многослойные нейронные сети. Мы с вами говорили про постановку задач машинного обучения с учителем, то есть я быстренько напомню, что у нас есть данные x и ответ к ним y, то есть, опять же, это пример с квартирами, и у каждой квартиры есть некие признаки, а есть ответ — это цена. И, допустим, нам нужно предсказать цену, то есть у нас есть x, у каждого x при этом есть признаки x1, x2, x3 и так далее, и есть y. Задачи машинного обучения строятся вот таким образом, нам нужно подобрать алгоритм A таким образом, чтобы получать отображение из x к y. Мы понимаем, что невозможно на всех данных сделать абсолютно точное отображение, поэтому мы вводим функцию потерь, которая показывает, насколько близко мы приближаемся к ответам, то есть это функция ошибки. Задача сводится к тому, что нам нужно минимизировать эту функцию ошибки, и делаем это, меняя параметры модели. Да, кстати, сейчас я посмотрю, видно ли, а то вдруг я перегородил чуть-чуть камеру. Окей, мы с вами успели поговорить про линейные модели. Кратенько повторю, что формула для линейной модели выглядит таким образом, то есть это матричная запись. Я напоминаю, что мы пользуемся матричными операциями, и вы сегодня увидите, почему именно ими. Функции для задачи регрессии — это MSE и MAE. Мы с вами говорили, как они выглядят, что они делают и как это делается, то есть мы делаем градиентный спуск. Чтобы посчитать градиентный спуск, нам нужно посчитать градиент. Мы обсуждали, что линейная регрессия выглядит вот таким образом. Транспонирование зависело от того, как вы задаете матрицы X, Y и так далее. Но в целом формула выглядит вот так. Мы говорили про задачу классификации, в частности, бинарной классификации. В классификации задача выглядит немного по-другому: нам нужно выделить прямую, которая разделяет объекты на два класса. Чтобы это сделать, мы вводим функцию сигмоиды, на вход которой подаем линейную комбинацию. Предсказанием будет вот такая функция. На вход подается линейная комбинация X на theta. У нас появляется промежуточная переменная Z, которая является этой линейной комбинацией. Класс функции для логистической регрессии выглядит вот таким образом. Здесь предсказания принимают значения от 0 до 1. Сигмоида стремится к нулю и стремится к единичке, и поэтому значения находятся в диапазоне от 0 до 1. Если значение 0, то мы получаем соответствующее значение нуля, если 1 — то и так далее. В всех классах функций, я напомню, мы усредняем по всем объектам из датасета. Это то, что было в прошлый раз. Теперь будем строить с вами нейронные сети. Давайте сначала я вам расскажу, откуда вообще растут ноги. Почему нейронные сети стали так популярны. На самом деле, первые модели нейронных сетей появились еще в 20 веке, но они были в основном теоретическими. Не было достаточно вычислительной мощности, чтобы обучать нейронные сети, нужны были данные и вычислители для их обучения. То есть необходимо было хранить эти данные. Поэтому даже в 90-х, когда компьютеры уже развивались, были проблемы с хранением данных. Я застал это время, когда дискеты могли хранить всего несколько мегабайт. Но начиная с нулевых, всё стало развиваться, и появились видеокарты. Искусственный интеллект нашел популярность в обучении нейронных сетей для распознавания образов на изображениях. Это сетка AlexNet. Раньше применяли классические алгоритмы компьютерного зрения, чтобы решать эти задачи. А тут просто взяли нейросеть, обучили, и она значительно превзошла по метрикам классические подходы. Люди поняли, что когда у вас есть много данных, качество нейронных сетей всё больше и больше растет. У классических алгоритмов машинного обучения качество выходит на некое плато, а у нейронных сетей нет таких ограничений. Мы видим, как OpenAI берет всё больше данных, и их качество только увеличивается. Это подход, который продолжают развивать. Теперь давайте поговорим о том, что такое нейронная сеть. Я вам показал, как выглядит именно сетка из нейронов, а вот что зашито в этот фиолетовый кружочек. На вход идут сигналы, которые представляют собой наши данные, то есть признаки. В случае с квартирами — это размер, район и другие метрики. Цена — это целевая переменная. Модель перцептрона состоит из двух частей: мы все сигналы суммируем и делаем это линейным образом, после чего применяем нелинейность, что называется функцией активации. Функция активации — это нелинейная функция. Мы посмотрим, какие функции активации существуют. Одной из них является Relu, которая при отрицательных значениях равна 0, а при положительных равна x. Если у вас будут вопросы, мы можем рассмотреть, какие функции активации применять. Давайте теперь посмотрим, как выглядит структура нейронных сетей. нужны полукси. Если у вас на выходе будет несколько значений, вы сможете использовать различные функции активации для разных слоев, но важно помнить, что это усложняет обратное распространение ошибки и вычисление градиентов. 

Теперь давайте вернемся к матричной записи. Мы видим, что для каждого нейрона есть своя матрица весов, которая связывает его со входами. Каждый нейрон принимает множество входных значений, умножает их на соответствующие веса и применяет функцию активации для получения выходного значения. 

Мы обсуждали, что выходы из одного слоя становятся входами для следующего. Например, значения на выходе из второго скрытого слоя становятся входами для выходного слоя, где мы применяем последний слой весов и функцию активации, чтобы получить финальные предсказания.

Если у нас выходной слой, скажем, из четырех нейронов, в зависимости от значений, которые они выдают, мы можем определить, к какому классу относится входное значение. Самый высокий выход будет указывать на наиболее вероятный класс. 

Теперь давайте я открою еще одну иллюстрацию, чтобы понять, как выглядят эти операции в действительности. Я вам покажу пример матричных операций, которые мы обсуждали, и как они применяются в нейронных сетях. Это поможет вам лучше понять разницу между линейной моделью и многослойной нейронной сетью, а также увидеть, как мы можем организовать веса в матрице, чтобы добиться более сложных функций и зависимостей. выходу, тем более сложные и осмысленные фичи формируются. Это связано с тем, что нейронные сети, обучаясь, создают иерархию представлений. 

Когда проходят все слои, выходные значения от нейронов ближе к выходу уже учитывают более сложные закономерности в данных, и это позволяет нейронной сети принимать обоснованные решения. При этом важно помнить, что инициализация весов и их обновление играют ключевую роль в процессе обучения. 

Для эффективного обучения нейронной сети необходимо следить за значениями градиентов. Если градиенты на некоторых слоях становятся слишком маленькими, это может вызвать проблемы, такие как затухание градиента, что особенно критично для глубоких сетей. Важно использовать правильные методы инициализации весов, чтобы избежать этого.

На следующем занятии мы рассмотрим разные техники инициализации весов, а также методы, которые позволяют улучшить сходимость градиентного спуска. Обращайте внимание на то, как изменение инициализации весов влияет на скорость и стабильность обучения модели. 

Если у вас есть вопросы по теме, пожалуйста, задавайте их. Также рекомендуется провести практическое занятие и попробовать реализовать нейронную сеть, учитывая все аспекты, которые мы обсудили. выходу, тем более что-то осмысленное. Это если не интуитивным образом рассказывать. Есть целые работы, которые изучают, как слои нейронных сетей, имеют ли они какой-то смысл, пытаются разобраться, как слои создают некие физические представления. Особенно я видел работы, связанные с LLM (large language models), но фишка в том, что чем больше у вас нейросеть, тем сложнее ее анализировать, потому что у нее много слоев и много параметров. Но некая интуиция в этом заключается. Что еще вам рассказать? В принципе, я рассказал, как устроены нейросети в целом. Получается, здание именно из-за этой архитектуры? Ну, не только из-за архитектуры, а в целом. Из-за идеи линии? В целом, да. Смотрите, я чуть подробнее расскажу. То есть там оба этих человека, с которыми дали Нобелевскую премию, один Кинтон, а другой Хопкинс, по-моему. И оба они стояли у истоков нейросетей. Один из них как раз предложил метод обратного распространения ошибки. Но обоим дали награду, грубо говоря, за то, что они предложили разные архитектуры, причем разработанные на основе физических инсайтов. То есть у них в голове крутилась некая физическая модель, они ее представили в виде математики и получили некоторые решения. Там модель Больцмана, еще какая-то модель и модель Хопкинса. То есть они получили свои идеи на основе инсайтов из физики. В итоге это стало существенным вкладом в развитие нейросетей. Поэтому, почему именно за физику дали награду, непонятно, но считается, что в формальном релизе написано, что они вывели свои нейросети на основе физических аспектов. Но сами нейросети стали активно применяться не только в физике, а в разных областях. В общем, вот если вкратце, то получается такая история. Значит, сейчас. Что я хотел сказать? Давайте последний момент скажу. Если вы захотите запрограммировать это своими руками, существует множество фреймворков, которые это делают. Тот же PyTorch в автоматическом режиме строит вычислительные графы и находит производные. Как это делать дальше, Егор на семинарах расскажет. Но, допустим, вы хотите попробовать сделать это сами. Чтобы сделать это самостоятельно, вам потребуется применить один лайфхак, про который я сейчас расскажу. Лайфхак заключается в том, что важно как можно меньше использовать циклы, потому что это сильно замедляет выполнение, а во-вторых, усложняет процесс. Например, возьмем простую математическую операцию: wx + b, где b – это столбец с числами. Если b равно 100, а вам нужно 4b, вы можете оставить это как 100, и он автоматически обработает это правильно в NumPy. Столбик приводит к такому результату. Поэтому, когда речь идет о различных операциях, таких как сложение, вычитание и умножение, не стоит использовать циклы. В том же NumPy это все делается автоматически. Или, например, в функции Sigmoid с экспонентами, вы просто задаете np.exp(-x) и можете сразу подать матрицу. Он применит экспоненты к каждому элементу матрицы, и вам не нужно писать цикл for. Это просто небольшие лайфхаки, которые облегчат вашу работу, если вы захотите это делать. Мы как раз близимся к завершению лекции. В принципе, у меня нет особых добавлений. Я призываю вас посмотреть дополнительные материалы и отдельные лекции. Закончим тем, что стоит понимать, откуда взялась тема нейронных сетей. Человеческий нейрон тоже имеет ядро, в которое приходят сигналы. Каждый нейрон связан с другим нейроном, и связь между ними осуществляется через различные механизмы. Приходят сигналы, они обрабатываются и уходят дальше. Это просто выходные значения. Но суть в том, что нужно понимать: это не прямая аналогия с реальными нейронами, потому что нейробиологи до сих пор, возможно, не совсем понимают, что происходит в ядре реального нейрона человеческого. Это просто математическая абстракция. И просто из-за подобия внешнего вида была придумана терминология нейронной сети, потому что здесь тоже нейроны связаны между собой, идут сигналы и нелинейности. Но нужно помнить, что нелинейности в этой модели гораздо проще, чем то, что происходит в реальности. Поэтому назвали это нейронной сетью. В целом, из этой лекции вам нужно вынести три ключевые вещи. Во-первых, как выглядит устройство нейрона, выглядит это таким образом. Дальше вам нужно понять, как происходит распространение сигнала слева направо. И, наконец, вам нужно разобраться, как работает механизм обратного распространения ошибки (chain rule). Если вы захотите подробно изучить это, посмотрите PDF-документ, там немного сложно, но можно найти видеозапись с этой лекции на YouTube. Мне лично для понимания очень помогла эта лекция, она из курса глубокого обучения. ЭтоDeep Learning, можете найти на YouTube. Это лекции, которые действительно подробно объясняют механизмы работы нейронных сетей. Сейчас покажу, о чем идет речь: он начинает с логистической регрессии и далее переходит к нейронным сетям, разбирая уравнения для каждого нейрона и показывая, как это все векторизуется. Он идет дальше и показывает векторизацию на весь датасет, а я вам показал только для одной строки. Это может быть сложно понять на первый взгляд, но тоже можно сделать. Если вам это будет интересно, посмотрите. Это действительно поможет вам разобраться в механизмах работы нейронных сетей. А соответственно, Егор вам будет далее рассказывать, как производить обучение и кодирование. Кажется, я все рассказал, если есть вопросы, спрашивайте. Если все было понятно, дайте знать. Если кто-то хочет пообщаться, приходите. На этом все на сегодня. Спасибо. До свидания.
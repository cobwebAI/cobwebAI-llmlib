Проблема линейных моделей в том, что как только у вас возникает нелинейность, здесь писать нельзя. Вы можете отпустить. Круппу говоря, представьте, что у вас задача классификации. Задача классификации выглядит вот таким образом. Вот у вас христики. Сейчас придумать бы еще. Нолики. Это я в признаковом пространстве. Я придумал как. Смотрите. Вот так вот нолики вокруг этой штуки. А тут внутри христики. Получается, что поверхность, которую я бы хорошо разделял, выглядит таким образом. И тут вы никак линейно это не сделаете. Поэтому линейная модель тут плохое качество. Поэтому, собственно говоря, нужно придумать новые инструменты. В следующем семестре вы пройдете такие алгоритмы, как метод влияющих соседей, градиентный бустинг, случайный лестник, который я говорил. Но они там тоже довольно специфичные. То есть вы увидите, что довольно специфичным образом линейные зависимости приближают. В этом плане нейронки, если их хорошо обучить и так далее, они более точно могут это сделать. Какая тут есть штука с этим связанная? Есть такая теорема, она чисто математическая. И суть ее заключается в том, что если у нас есть какая-то функция, и с какой точностью ее нужно приблизить, мы всегда с этим справимся. Даже однослойной нейронной сетью. То есть мы сможем подобрать такую нейронку веса, что она любую нелинейную зависимость аппроксимирует. Вы уже здесь можете видеть, что такое нейронная сеть. У нас есть входной слой, промежуточный слой и выходной слой. И тут видно, что этот сигнал идет в каждый нейрон. Этот сигнал тоже идет в каждый нейрон, поэтому тут много связи получается. Чисто математически было доказано, что если у нас есть какая-то функция абстрактная, мы можем ее аппроксимировать однослойной нейронной сетью. Теперь давайте посмотрим, что такое нейронная сеть. Я вам показал, как выглядит сетка из нейронов, а что зашито в этот фиолетовый кружочек. Фиолетовый кружочек саши – это такая штука. Что здесь происходит? У нас на обход идут какие-то сигналы. Сигналы – это наши данные, это признаки. В случае с квартирой – это размер, район. Цена метра? Нет, цена – это был таргет. Это то, что мы представили. Это то, что здесь у нас было. А здесь внутри. И это входные сигналы. Они на вход идут с какими-то весами. В1, В2 и так далее. И еще есть у нас по-прежнему сдвиг, В0 некий. Это все суммирует. Модель перцептрона состоит из двух частей. Первая часть – это мы все сигналы суммируем и делаем это линейным образом. Значит, вес – это приоритет? Вес – это как линейная модель, мы взвешиваем сигналы. В этом плане это совпадает с линейной моделью. А вот дальше ключевой момент – это то, что мы применяем нелинейность. Это называется функция активации. Функция активации – это какая-то нелинейная функция. Мы сейчас с вами посмотрим, какие бывают. Суть в том, что именно эта нелинейность позволяет нам аппроксимировать нелинейные зависимости. Давайте теперь посмотрим, какие функции активации бывают. Мы с вами уже видели, что в логистической регрессии применяется сигнуид. Но на самом деле есть разные функции активации. Есть гиперболический тангенс, который от минус одного к одному стремится. Есть вот такая функция, называется… А что ж ты тут делаешь? Он, мне кажется, перегревается просто. Ладно, пока он думает, давайте уж как-то… Я могу просто разворачивать вот это. Ну, давайте я быстро тогда… Пусть так будет. Вам видно, нет? Короче, если что, поближе подходите пока. В общем, у нас функция активации, которая называется relu. Ее суть в том, что при отрицательных значениях она ровно 0, а при положительных, собственно, равна просто x. То есть это прямая под 45 градусов. Мы с вами обсуждали, что тут есть две проблемы. Во-первых, в нуле вот такой излом. И теоретически там может быть проблема со взятием производных. Но мы с вами обсудили, что на практике обычно этой проблемы не бывает. Тут проблема еще в том, что у вас при отрицательных значениях нулевые значения. И получается, что вы, по сути, много сигналов просто обрубаете. Потому что если они на вход нейрона идут отрицательные, то вы их просто обнуляете. И иногда, если вам хочется, чтобы их не обнулять, то можно применить ли кириллу, так называемый. То есть это, когда у вас не ноль, а другая прямая под другим углом. То есть у вас, получается, меньше нуля – это какой-то один угол, а больше нуля – другой угол. Ну и есть всякие разные. Есть елу. Короче, их довольно много придумали. Но самые основные, которые не ровно применяются – это рилу, сигмойда. Я вам уже рассказывал, какие там могут быть проблемы. Я сегодня еще повторю. Гиперболический тангенс бывает. В принципе, любой из них можно применять и смотреть, что у вас получается. Сейчас я попробую переспустить. Пока она думает. Собственно говоря, мы с вами сейчас структуру нейронов посмотрим. Но я давайте все-таки выведем сейчас. Мы с вами обсудим, как сигнал распространяется в нейронных сетях. Давайте пока он думает. Такой вопрос на засыпку. А что будет, если у нас не будет функции активации? То есть если мы просто суммируем все сигналы… Давайте вот такой вопрос на засыпку. Что будет, если мы просто уберем из перцептрона функцию активации? Отсюда вот этого вообще уберем. Оставим только вот это. Будет просто линейная функция. Даже если у вас будет много каких-то нейронов, но вы всех уберете функцию активации, у вас просто все линейные комбинации друг к другу будут складываться и останутся линейные комбинации. Это к тому, что функция активация – это ключевая вещь, которая нам позволяет делать аппроксимацию нелинейных зависимостей. Обучение. Сейчас я вот тут буду щелкать иногда другим материалом. Давайте посмотрим. Это мы с вами уже проходили. Рассказываю четко. Соответственно, логистическая регрессия, как она выглядела еще раз, что у нас есть линейная комбинация, но мы берем сигнольду и дальше применяем такую функцию. В случае нейронной сети у нас получается, что мы можем взять много нейронов. То есть если в логистической регрессии у нас было только одно место, куда стекались все… Я хотел сравнение с логистической регрессией, как это выглядит. Представьте, что в логистической регрессии у нас получается, что по-прежнему красный входной сигнал стекается в одно место, и у вас output layer. Получается, что в логистической регрессии у вас не будет вот этого, и в фиолетовых будет только один кружочек. То есть вот в этом разница получается. А в многослойных нейронных сетях у вас может быть несколько слоев и много нейронов в каждом слое. То есть еще раз, давайте, знаете, как сделаем. Вот так вот сделаю. И получается, да, вот картинка внизу. Видите, получается, что здесь все стекается в одно место, и там применяется сигмой. А здесь, видите, у нас получается много связей, много нейронов, много слоев. В данном случае два слоя. Вот в этом отличие. Продолжаем здесь. Да, может быть на самом деле много выходов. Вот придумайте задачу, где может быть много выходов. Давайте, кто может предлагать? Есть решение? Несколько выходов, да, то есть у нас output var, noval. Классификация? Да. Может быть, например, самая простой, наверное, компьютер вижн, который предъявляет кофе, собачку? Да, то есть вот есть датасет, который называется ImageNet. Там тысяча классов и, соответственно, тысяча вот таких выходных нейронов. Ну, не нейронов, а выходов. Теперь давайте с вами рассмотрим прямое распространение. Сейчас нужно будет немножко напрячься, понимая, что уже конец дня и сложно. Но давайте верхний уровень посмотрим, как это выглядит. То есть у нас, получается, есть... Ну, будем на таком участке нейронной сети смотреть. То есть мы сейчас не будем всю нейронку смотреть, а просто участок. То есть вот у нас есть, грубо говоря, некий слой. Мы его назвали Admin.
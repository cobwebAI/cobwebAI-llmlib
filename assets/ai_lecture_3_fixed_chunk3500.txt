Запись начала, отлично, теперь точно можно начинать. Мы сегодня с вами говорим про нейронные сети и наконец-то переходим к архитектуре нейронных сетей, начнем с самой простой модели – перцептрона. Из перцептронов мы будем строить многослойные нейронные сети. Мы обсуждали постановку задач машинного обучения с учителем: у нас есть данные x и соответствующие им ответы y. Например, если говорить о квартирах, то каждый объект – это квартира с определенными признаками, а ответом будет цена. Нам нужно предсказать цену, то есть на основе x, у которого есть признаки x1, x2, x3 и так далее, получить y. 

Задачи машинного обучения строятся так, что нам нужно подобрать алгоритм A, обеспечивающий отображение из x в y. Понимаем, что невозможно получить абсолютно точное отображение на всех данных, поэтому вводим loss-функцию, которая показывает, насколько близко мы приближаемся к ответам, то есть это функция ошибки. Наша задача – минимизировать эту функцию ошибки, изменяя параметры модели. 

Мы успели поговорить про линейные модели. Формула для линейной модели выглядит в матричной записи. Мы используем матричные операции, и вы сегодня увидите, почему именно ими. Функции для задачи регрессии – это MSE и MAE. Мы говорили о том, как они выглядят и что они делают. Для расчета градиентного спуска нам необходимо вычислить градиент. Мы обсуждали, что линейная регрессия выглядит вот так. Транспонирование может зависеть от того, как вы задаете матрицы X, Y и так далее.

Мы также говорили о задаче классификации, в частности, бинарной классификации. Классификация выглядит немного иначе: нам нужно выделить прямую, разделяющую объекты на два класса. Для этого вводим функцию сигмоиды, на вход которой подаем линейную комбинацию. Предсказания Y будут функцией, исходящей от этой линейной комбинации. У нас появляется промежуточная переменная Z. Класс функции для логистической регрессии также выглядит определенным образом. Предсказания принимают значения от 0 до 1. 

Когда выстраиваются различные модели, линейная модель сталкивается с проблемами, когда возникает нелинейность. Линейная модель не способна хорошо справляться с такими зависимостями. С точки зрения линейной модели, она может дать лишь приблизительное решение, в то время как нейронные сети, если их правильно обучить, могут более точно его найти. 

Исторически первые модели нейронных сетей появились в 20 веке, однако они были в основном теоретическими из-за нехватки вычислительных мощностей и данных для обучения. С начала 2000-х годов развитие видеокарт и доступ к большим объемам данных стали причиной популяризации нейронных сетей. Обучение нейронных сетей для распознавания образов, как, например, в случае с сеткой AlexNet, продемонстрировало, что нейронные сети значительно превосходят классические алгоритмы по метрикам. С увеличением объема данных качество нейронных сетей продолжает расти, в то время как классические алгоритмы приходят к плато.

Теперь давайте перейдем к нейронным сетям. Проблема линейных моделей заключается в том, что они не способны адаптироваться к нелинейным зависимостям. Теперь мы можем увидеть, как выглядит задача классификации. Например, если у нас есть признаки, мы можем нарисовать точки в признаковом пространстве. Но если нам нужно разделить их на два класса, прямую провести будет невозможно. 

Следующие методы, такие как методы ближайших соседей, градиентный бустинг, случайные леса, также имеют свои специфические ограничения. Нейронные сети, если их правильно обучить, могут более точно аппроксимировать даже сложные зависимости. Существенно, что математически доказано, что любая функция может быть аппроксимирована с помощью нейронной сети.

Фиолетовый кружочек обозначает основную идею: на вход нейронной сети идут сигналы, которые представляют собой признаки. Например, в случае с квартирами признаки могут включать размер, район и другие параметры, а цена – это целевой показатель. Мы будем суммировать сигналы с определенными весами и добавлять смещение. Модель перцептрона включает две части: линейное суммирование всех сигналов. Вес – это коэффициент, позволяющий учитывать важность каждого признака. Это совпадает с линейной моделью. А вот дальше ключевой момент – это то, что мы применяем нелинейность, то есть функция активации. Функция активации – это какая-то нелинейная функция. Мы сейчас с вами посмотрим, какие бывают функции активации. Суть в том, что именно эта нелинейность позволяет нам аппроксимировать нелинейные зависимости. 

Давайте теперь посмотрим, какие функции активации существуют. Мы уже видели, что логистическая регрессия применяется с сигмоидой, но на самом деле есть разные функции активации. Есть гиперболический тангенс, который стремится от минус одного к одному. Есть функция ReLU, ее суть в том, что при отрицательных значениях она равна 0, а при положительных равна просто x, то есть это прямая под 45 градусов. 

Мы обсуждали, что у этой функции есть две проблемы. Во-первых, в нуле возникает излом, что может привести к проблемам с взятием производных. Но на практике часто этой проблемы не бывает. Однако, при отрицательных значениях получается, что много сигналов просто обрубается. Если они на вход нейрона идут отрицательные, то вы их просто обнуляете. Чтобы избежать этого, можно применить модификацию ReLU, известную как Leaky ReLU. При этом у вас для отрицательных значений будет другой угол наклона. 

Существует множество других функций активации, таких как ELU и другие. Но основными, которые чаще всего применяют в нейронных сетях, являются ReLU, сигмоида и гиперболический тангенс. Я повторю, что любой из них можно применять и наблюдать результаты.

Теперь давайте поговорим о том, как сигнал распространяется в нейронных сетях. Что будет, если убрать функцию активации? Если мы просто суммируем все сигналы, то у нас получится линейная комбинация. Даже если нейронов будет много, но без функции активации все линейные комбинации останутся линейными. Функция активации – это ключевая вещь, позволяющая аппроксимировать нелинейные зависимости.

Теперь перейдем к обучению нейронных сетей. В логистической регрессии у нас была линейная комбинация, но с применением сигмоиды. В нейронной сети мы можем взять много нейронов, а не один, как в логистической регрессии. Сигнал из каждого нейрона будет стекаться в выходной слой, и это ключевое отличие: в многослойной нейронной сети могут быть несколько слоев и множество нейронов в каждом слое.

Нейронные сети могут иметь много выходов. Например, в задачах классификации, таких как различие между кошками и собаками, часто используются датасеты типа ImageNet, которые содержат тысячи классов и соответственно множество выходных нейронов. 

Теперь давайте рассмотрим прямое распространение. Мы рассмотрим участок нейронной сети. Есть некий слой, мы его обозначили как ht-1, и ht – это выходной слой. Нам нужно посчитать выходы на основе значений, которые находятся в этом слое. Мы домножаем веса на значения нейронов и добавляем смещение. Обратите внимание, что матрица V появляется из-за наличия множества нейронов и связей с предыдущим слоем.

Таким образом, у нас появляется различие между нейронной сетью и простой линейной моделью. В нейронной сети у каждого нейрона есть свои связи, и это позволяет учитывать больше информации. Поэтому матричные операции очень важны в нейронных сетях, так как они позволяют сгруппировать вычисления.

Вес, передаваемый нейронами, может быть разным для каждого выхода. Эти веса подбираются в процессе обучения, что делает каждую нейронную сеть уникальной. Важно понимать, что каждый нейрон агрегирует значения и превращает их в одно число с помощью функции активации.

В целом, это и есть принципы работы нейронных сетей. Каждая нейронная сеть состоит из слоев, на каждом из которых происходят линейные и нелинейные операции, что позволяет моделировать сложные зависимости в данных. Понятно. Может быть, это связано с тем, что он переходит в этот нейрон? В плане, почему v1? Давайте сейчас сначала разберемся: x — это желтые значения, это все желтые значения. Это x. Значит, дальше, этот x с весами b0. Обратите внимание, b0 означает, что b0 учитывает все веса, связанные с этими линиями. И плюс ко всему, есть некий b0, который тоже учитывает все значения сразу. Это получается матричная запись, и f0 — это некая функция активации, которая применяется здесь. В результате выхода из этого мы получаем значение для этого слоя, для слоя 1. 

Дальше мы используем значение этого слоя, чтобы получить значение для следующего слоя. Итак, чтобы посчитать значение этого слоя, мы домножаем значения на матрицу b1, которая учитывает все веса, связанные с этим. И в результате применения f1 получаем значение здесь. Теперь, чтобы получить значение следующего слоя, мы домножаем значения, которые получились здесь, на b2, которая учитывает всё, что здесь, и получаем соответствующее значение. И, наконец, мы домножаем на b3 для выхода из этого слоя и получаем конечное значение. В итоге, если всё это правильно сделать, мы получаем выходные значения. 

Обратите внимание, что это математическая запись. Я хотел подробно вам это рассказать, потому что чтобы получить, например, значение скрытого слоя 2, вам нужны значения скрытого слоя 1, которые, в свою очередь, получаются из значений предыдущих слоев. То есть это как матрешка: чем дальше идете, тем более сложной становится запись. Но суть остается простой — вы используете значение предыдущего слоя для вычисления значения текущего слоя. 

Функция активации, да, это функция активации. На выходе это может быть, например, ReLU или Softmax для последнего слоя. 

Теперь, почему мы используем матрицу? Ну, на выходе — четыре выхода. Это можно реализовать с использованием соответствующих библиотек. Да, там четыре значения, которые можно использовать для вычисления. Вы можете взять наибольшее значение, которое будет соответствовать предсказанному классу нейронной сети, например, кошке. Вы это увидите на семинаре. В общем, вы смотрите значение, и наибольшее значение соответствует классу. 

Теперь, что касается V3. Если размерность V3 будет равна 4, вы получите четыре значения. Это правильно. И поэтому у вас будет своя матрица весов и функция активации, которая зависит от того, как вы ее зададите. Обычно в рамках одного слоя задают одну и ту же функцию активации, но для разных слоев могут быть разные принципы. Хотя обычно так не делают, потому что это усложняет обратное распространение ошибки и вычисление градиентов.

Чтобы обучить нейронные сети, сначала нужно сделать прямое распространение сигнала, а затем обратное. Вы получаете выходной сигнал, и в процессе нужно понять, в каких местах рассчитывать производные. Поэтому мы сохраняем все промежуточные значения, чтобы использовать их при обратном распространении ошибки. В любом случае, когда вы обучаете нейронки, вы много раз выполняете одно и то же: берете сигнал, распространяете его, запоминаете промежуточные значения, считаете функцию потерь и затем начинаете вычислять производные. 

Этот метод называется backpropagation, то есть обратное распространение ошибки. Он заключается в том, что вы вычисляете выходной сигнал, затем проходите весь процесс обратно и берете производные на пути для применения градиентного спуска. Таким образом, вы и обновляете веса. 

Если у вас есть вопросы, задавайте их. производную. Если вы будете использовать метод обратного распространения ошибки и цепочку для вычисления производных, вам нужно сохранять промежуточные значения. Без этих значений вы не сможете считать градиенты. Вы, по сути, создаете некий кэш, который переиспользуете при вычислении градиентов. Вот и здесь это видно. Мы движемся справа налево, учитывая все веса, смещения и выходные значения, сохраняя промежуточные результаты, когда считаем градиенты. Здесь появляется dA, это и есть градиент. Вы считаете градиенты, и на основе этого производите обновление параметров. Как только вы получили градиенты, вы выполняете обратное распространение ошибки. Этот процесс повторяется много раз, пока вы не достигнете желаемых результатов.

Теперь возникает вопрос о функции активации. Обратите внимание, что для всего слоя используется одна функция активации, а для выходного слоя, например, применяется сигмоида, если у вас бинарная классификация. Выисчисляете значение потерь и ошибку по функциям потерь.

Вот здесь представлена формула для моделирования нейронных сетей, которую мы рассматриваем. Функция активации — это бинарная логистическая функция потерь. Чтобы посчитать производную, используется значение a. Мы получаем это значение, когда движемся слева направо. Я не стал подробно вводить формулы, но механизм я изложил. Обратите внимание, что это везде матричные операции, так как у вас много весов и параметров. Для каждого нейрона не имеет смысла расписывать отдельно.

Теперь о том, как инициализировать веса. В нейронной сети, когда она еще не обучена, веса должны быть заданы. Можно легко задать их нулями, но в этом случае возникнет проблема симметричных градиентов, что приведет к плохому обучению. Если все веса равны нулю, выходные значения могут оставаться маленькими, что может вызвать затухание градиентов. Проблема затухания градиентов возникает, когда каждый из градиентов маленький, и произведение будет ещё меньше.

Если у вас градиенты близки к нулю, они не будут обновляться. Это и появляется в глубоких нейронных сетях, где риски обучения глубже слоев меньше, чем в выходных. Поэтому инициализация нулями — это плохая идея. Я расскажу о лучших методах. Например, вы можете использовать случайное равномерное распределение весов, но с небольшим значением, чтобы не было слишком великих весов.

В нейронных сетях более глубоких уровней могут возникать более простые функции в решении задач, чем на выходных слоях. Выходы из более глубоких нейронов представляют простые функции, тогда как ближе к выходу они становятся более осмысленными. Исследуются и анализируются эти слои. Важно понимать, что чем больше слоев, тем сложнее анализировать нейронную сеть.

Если вы захотите реализовать нейронную сеть самостоятельно, то знайте, что стоит избегать избыточного использования циклов, это замедляет и усложняет код. Пользуйтесь библиотеками, такими как NumPy, что позволяет работать с матрицами более эффективно, без необходимости писать циклы для каждой операции.

Таким образом, подводя итог, вам нужно вынести основные моменты: структуру нейрона, распространение сигнала слева направо и механизм обратного распространения ошибки. Если вы хотите более подробно ознакомиться с формулами, рекомендую смотреть лекции на YouTube, особенно те, которые читает Андрю Анджи. Он действительно подробно анализирует каждую структуру нейронной сети. Векторизация на весь датасет, а я вам показал всего лишь для одного элемента из датасета. То есть, я вам, по сути, продемонстрировал векторизацию для одного элемента. И, конечно, все эти процессы должны быть выполнены для каждого элемента из датасета. Это может быть сложно, поэтому не стал вникать в детали, но это тоже можно сделать. Здесь подробно об этом говорят. Если вам это будет интересно, посмотрите лекцию, чтобы разобраться в механизме работы нейронных сетей. Егор расскажет вам, как это программировать и обучать.

Если у вас есть вопросы, давайте обсудим, потому что в целом я рассказал все, что хотел. Поставьте плюсы, если все было понятно. Если кто-то хочет со мной пообщаться, пожалуйста, приходите. На этом всё на сегодня. Спасибо. До свидания.
Всё, запись начата. Отлично. Теперь точно можно начинать. Так вот, быстро повторюсь. Мы сегодня с вами говорим про нейронные сети. Наконец-то переходим. Начнём с самых простых объектов архитектуры нейронных сетей. То есть это модель перцептрона. Из перцептронов мы будем делать с вами многослойные нейронные сети. Соответственно, мы с вами говорили про постановку задач машинного обучения с учителем. Да, то есть я быстренько напомню, что вот у нас есть данные x и ответ к ним y. То есть, опять же, это пример с квартирами, что у нас каждая строка – это некий объект квартиры, у каждой квартиры есть некие признаки и есть ответы, это цена. И, допустим, нам нужно предсказать цену, то есть у нас есть x, у каждого x при этом есть признаки x1, x2, x3 и так далее, и есть y. И, соответственно, Задача машинного обучения строится вот таким образом, то есть нам нужно подобрать алгоритм А таким образом, чтобы получать отображение из X в Y. При этом мы понимаем, что невозможно на всех данных сделать абсолютно точное отображение, поэтому мы вводим loss-функцию, которая нам показывает, насколько близко мы приближаемся к ответам, то есть это функция ошибки. И задача сводится к тому, что нам нужно минимизировать эту функцию ошибки, и делаем это мы, меняя вот эти параметры модели. Мы с вами успели поговорить про линейные модели, да, опять же, кратенько повторю, что формула для линейной модели выглядит таким образом, то есть это матричная запись, да, я напоминаю, что мы пользуемся матричными операциями, и вы сегодня увидите, почему именно ими. Значит, получается, что у вас функция для задачи регрессии — это mse и mya. Мы с вами говорили, как они выглядят, что они делают, и, собственно, как это Делается, то есть мы делаем градиентный спуск, да? И чтобы посчитать градиентный спуск, нам нужно посчитать, собственно говоря, градиент. Вот он здесь, да? И мы обсуждали, что линейный регрессор выглядит вот таким образом. Правда, там как бы транспонирование у нас было вот здесь, но это зависит от того, как вы задаете матрицы x, y и так далее. То есть в этом плане это неважно, просто зависит от того, как вы задаете матрицы. Но в целом формула выглядит вот таким образом. И говорили с вами про задачу классификации, в частности, бинарной классификации. И говорили, что в классификации немножко по-другому выглядит задача. Нам нужно вот эту прямую выделить, которая разделяет объекты на два класса. И чтобы это сделать, мы вводим такую функцию сигнольда. На вход этой функции подаем линейную комбинацию. И предсказаниями тогда y будет вот такая функция. Этой функции подается линейная комбинация x на y. То есть у нас появляется промежуточная переменная z, которая является этой линейной комбинацией. И лоз-функция для логистической регрессии выглядит вот таким образом. Здесь у нас по благорифму вот эти самые предсказания. При этом кто-то мне напомнит, вот эти предсказания какие значения принимают в таком виде, которые после сигмой? Это единичка. Да, ну правильно, да, то есть это просто вот сигмуэль выглядит таким образом, она стремится к нулю, стремится к единичке, и получается, что от нуля до единицы. И поэтому вот здесь вот будут значения от нуля до единицы, да, ну и здесь один минус, да. А вот здесь вот стоят либо ноль, либо один, да. Соответственно, если ноль, то вот это обнуляется, да, и остается вот это вот. Если y равен 1, то вот это обнуляется, и остается только вот это вот. Во всех класс-функциях я напомню, что мы усредняем по всем объектам из дата-сета, то есть везде стоит сумма по всем объектам из дата-сета. Вот это то, что было в прошлый раз, и мы отталкивались от этого, и теперь будем строить с вами нейронные сети. Давайте сначала я вам расскажу, откуда вообще растут ноги, почему нейронные сети стали вот так популярны. На самом деле вы увидите, что... Первые модели нейронных сетей были еще в 20 веке, но они были в основном теоретические, то есть не было достаточно железа, вычислителей. Чтобы нейронные сети обучать, нужны данные и вычислители, чтобы их обучать с нейронной сетью. То есть нужно где-то хранить эти данные и их обучать. Поэтому даже в 90-х, когда вроде бы уже компьютеры развивались и стали напоминать то, что мы сейчас видим. Все равно была проблема именно с хранением данных. То есть вы помните вот эти дискеты, которые несколько мегабайт можно было хранить. Это еще в 90-х было, я застал в это время. А вот уже начиная с нулевых, вот это стало развиваться, и появились видеокарточки. И вот тут как раз Там, где искусственный интеллект нашел хайп — это обучение нейронных сетей для распознавания образов на изображениях. Это сетка AlexNet. В общем, эта сетка... В чем прикол? Раньше применяли классические алгоритмы компьютерного зрения, чтобы решать эти задачи, а тут взяли просто нейронку обучили, и она кратно превосходила по метрикам классические подходы. И вот тут вот люди поняли, что вот эту картинку, по сути, да, то есть, что когда у вас есть данные, много данных, чем больше вы подаете данных, тем у вас качество нейронов сетей все больше и больше растет, а у классических алгоритмов машинного обучения, которые вы будете проходить в следующем семестре, они выкладывают их качество, выкладывают на некое плато. И, соответственно, получается, мы даже сейчас это видим, да, то есть вот какой-нибудь OpenAI берет еще жирнее модели, еще больше данных, у них качество все растет и растет. Так они уже весь интернет продаются. Ну, короче, они продолжают этот подход. Он заморгал, может быть. Так, сейчас. А что с ним? Он перегревается просто? Понятно. Но пока вас не было, только видели. Видели чудо. Блин, ну что делать? Я могу, в принципе, просто развернуть монитор. Компану. Давайте я договорю мысль. В общем, суть в том, что на нейронке они в этом плане, у них меньше ограничений по качеству, вы на них можете подавать больше данных, и их качество будет все лучше и лучше. И, собственно, когда в нулевые годы появились видеокарточки По сути, начиная там с нулевых 2010-го. По-моему, прошлую эту субботу было 25 лет ровно, когда тебя там привезли. Ну вот, это сейчас скажу. Получается, какой год-то, 1999. Знаете что, я вам даже покажу видос. Сейчас подойдите, перейдите. То есть, как мы говорим, Нью-Йорк, это я мультиплеер. Сейчас фото. Еще раз вопрос. То есть сами нейронки это именно деберлинг или еще какие-то инстанции? В основном да, я имею ввиду деберлинг. То есть по сравнению с этими машинами, которые нейронят? Ну, потому что давайте так, в принципе, алгоритмы линейной реверсии известны очень давно и давно использовались. На самом деле, весь этот хайп про Data Science и так далее, я приводил личный пример. У меня магистрская диссертация была посвящена задачке, которая связана была с обработкой данных на большом адронном коллайдере. И там повсеместно применяются вот эти статистические подходы для обработки данных. И оказывается, это Data Science и Machine Learning. Вот эти подходы, мне кажется, были, грубо говоря, известны давно, и их применяли гораздо раньше, чем нейронки. Нейронки стало возможным применять именно из-за того, что возник вот эта возможность их обучать и хранить много данных. Ну, данные стало возможным, в принципе, тоже хранить чуть раньше, а именно применять видеокарты, чтобы их обучать, стало возможным относительно недавно. Поэтому, собственно, они вот так стрельнули вот в последнее время, и это все только набирает обороты. Вот. Собственно, да, ну да, тут, короче, я сразу скажу, у меня, как бы, слайды, они немножко недоделаны, потому что мы в процессе еще, как бы, в процессе того, что делаем курс, но я буду иногда переключаться сюда. Кстати говоря, вот эти материалы, это вот курс на курсере, но можно на ютубе найти видеозаписи выложенного. Там прям и очень много подчеркнуло отсюда. Короче, имею ввиду, да. Ну так вот, в чем проблема линейных моделей? Кто мне скажет? Можно предсказание линейной зависимости. Не все адаптируется. Ну, собственно, как только у вас возникает нелинейная зависимость, у вас нелинейная модель не очень хорошо работает. Вот тут пример, например, вот нелинейность, да, то есть, например, тут вот цена, да, сейчас, что тут, я про сайз, а, ну тут зависимость этого, цены от размеров, да, и видно, что здесь 0000, а потом с какого-то момента начинает расти. И тут возникает нелинейность. И получается, что в архилинейной модели здесь... Ну, ее там... Наверное, можно как-то вот так построить, да? Но вот здесь возникает нелинейность, да? Вот. И здесь как бы приводится пример, да, что у вас вот есть разные фичи, да? И вы их можете подавать. независимо друг от друга находят, и они дальше могут идти в такие штуки. Но это мы сейчас отдельно с вами поговорим, короче говоря. Поэтому сейчас не буду на этом останавливаться. Про обучение с учителем я говорил. То есть, с какими данными можно применять нейронные сети? табличные данные, аудио, картинки, текст. Это вы и так знаете, поэтому это мы быстро пропустим. Вот та самая картинка, которую я показывал. То есть здесь, кто не видит, здесь традиционного алгоритма. Здесь какие-то нейронные сети. А, и тут размеры нейронных сетей. То есть это маленькие, средние, большие архитектуры. И видно, как у них меняется качество в зависимости от количества данных, которые вы туда подаете. Вот. Собственно говоря, здесь показано, ну, как выглядит работа в Deep Learning, то есть у вас появляется идея, вы ее забиваете в код, проводите эксперимент, получается обратную связь, и у вас возникает на основе этого новая идея, то есть это такой очень довольно практичная область, ну, по многому экспериментальная. Вот. И давайте мы с вами Давайте от логистической регрессии с вами будем переходить к нейронкам. Проблема линейных моделей в том, что как только у вас возникает нелинейность, здесь писать нельзя, получается. Ладно. Вы можете поднять проектируем маркер на вкладке. В кнопке поднятия у вас левая плеча считается. Лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево-лево- Сейчас придумать бы еще как-нибудь, чтобы прям нолики. Ну, это я в признаковом пространстве, да? И как-нибудь... А, я придумал как. Смотрите, сейчас. Вот так вот нолики, то есть как бы вокруг этой штуки, да? А тут внутри крестики. То есть получается, что вот это разделяла, выглядит таким образом. И тут вы никак как бы линейно это не сделаете. Поэтому линейная модель тут как бы плохое качество. Поэтому, собственно говоря, нужно придумать новые инструменты. В следующем семестре вы пройдете такие алгоритмы, как метод ближайших соседей, градиентный бустинг, случайный лест, о котором я говорил. Но они там тоже довольно специфичны, то есть они, вы увидите, что довольно специфичным образом нелинейные зависимости приближают. В этом плане нейронки, опять же, если их хорошо обучить и так далее, они более точно могут это сделать. Вот какая тут есть штука с этим связанная? Вот есть такая теорема, она чисто математическая, и суть ее заключается в том, что вот если у нас есть какая-то функция, и с какой точностью ее нужно приблизить, мы всегда с этим справимся, даже от названия нейронной сети. То есть мы сможем подобрать такую нейронку небеса, что она любую нелинейную зависимость аппроксимирует. То есть вы уже здесь можете видеть, что такое нейронная сеть. То есть у нас есть входной слой, тут промежуточный слой и выходной слой. И тут видно, что, например, вот этот сигнал, он идет в красный нейрон. Значит, вот этот сигнал тоже идет в каждый нейрон, поэтому тут вот так много связи получается. Короче говоря, чисто математически было доказано, что если у нас есть какая-то функция абстрактная, мы можем ее аппроксимировать однослойной нейронной сетью. Теперь давайте посмотрим, что такое нейронная сеть. Я вам показал, как выглядит именно сетка из нейронов. Вот что зашито в этот фиолетовый кружочек? Ну, в фиолетовый кружочек зашита такая штука. Значит, что здесь происходит? То есть, у нас на обход идут какие-то сигналы. Сигналы – это, в принципе, наши данные, то есть, это признаки, да? В случае с квартирой – это там размер, там что там было, район, там вот это вот всё. Ну, короче… Ценометры. Что? Ценометры. Нет, цена — это был таргет. Это то, что мы представили, да. То есть это то, что здесь такой идейка у нас была. Вот, а здесь именно прицепы. И это входные сигналы. Значит, они на вход идут с какими-то весами. Там В1, В2 и так далее. И еще есть у нас по-прежнему сдвиг, В0 некий, да. И это все суммируется. То есть модель перцептрона, она состоит из двух частей. Первая часть — это мы все сигналы суммируем. И делаем это линейным образом. Что значит «вес да приоритет»? Вес да, это как линейная модель, мы взвешиваем сигналы. В этом плане это совпадает с линейной моделью. А вот дальше ключевой момент это то, что мы применяем нелинейность, то есть это называется функция активации. Функция активации это какая-то нелинейная функция, мы сейчас с вами посмотрим, какие бывают. Но суть в том, что именно эта нелинейность позволяет нам Аппроксимируется, собственно говоря, не вне независимости. Давайте теперь посмотрим, какие функции активации бывают. То есть мы с вами уже видели, что логистическая регрессия применяется с игноида, но на самом деле есть разные функции активации. Есть гиперболический тангенс, который от минус одного к одному стремится. Значит, есть вот такая функция, называется... А что с тобой сделать? Это он, мне кажется, перегревается просто. Ладно, пока он думает, давайте уж как-то... Я могу просто разворачивать вот это. Ну, давайте я быстро тогда вот, пусть так будет. Вам видно, нет? Ну, короче... Если что, поближе подходите пока. В общем, у нас функция активации, которая называется relu. Ее суть в том, что при отрицательных значениях она ровно ноль. А при положительных, собственно, равна просто x, то есть это прямая под 45 градусов. Мы с вами обсуждали, что тут есть две проблемы. Во-первых, в нуле вот такой излом, и теоретически там может быть проблема со взятием производных. Но мы с вами обсудили, что на практике обычно этой проблемы не бывает. Еще в том, что у вас при отрицательных значениях нулевые значения, и получается, что вы по сути много сигналов просто обрубаете, если они на вход нейрона идут. отрицательные, то вы их просто обнуляете. И иногда, если вам хочется, чтобы их не обнулять, то можно применить ли-кириллу, так называемый. То есть это, когда у вас не ноль, а другая прямая под другим углом. То есть у вас, получается, меньше нуля – это какой-то один угол, а больше нуля – другой. Есть всякие разные. ЕЛУ. Короче, их довольно много придумали, но самые основные, наверное, которые не ровно применяются, это РЕЛУ, СЕГМОИДА. Я вам уже рассказывал, какие там могут быть проблемы. Я сегодня еще повторяю. Гиперболический тангенс бывает. В принципе, любой из них можно применять и смотреть, что у вас получается. Сейчас я попробую перезапустить. Пока она думает, собственно говоря, мы с вами сейчас структуру нейронок посмотрим, но я давайте все-таки выведем сейчас. Мы с вами обсудим, собственно, как сигнал идет у нас, как у нас сигнал распространяется в нейронных сетях. Давайте пока он думает. Такой вопрос на засыпку. А что будет, если у нас не будет функции активации? То есть если мы просто суммируем все сигналы... Давайте вот такой вопрос на засыпку. Что будет, если мы, соответственно, Просто уберем из перцептрона функцию активации. Вот отсюда. Вот этого вообще уберем. Оставим только вот это. Вот, да. будет просто линейная функция, даже если у вас будет много каких-то нейронов, но вы у всех уберете функцию активации, у вас просто все линейные комбинации друг к другу будут складываться и останутся линейные комбинации. Это к тому, что функция активация — это ключевая вещь, которая нам позволяет делать аппроксимацию нелинейных зависимостей. Окей, собственно, обучение, да? Значит, давайте, сейчас я вот тут буду щелкать иногда другим материалом. В общем, давайте посмотрим. Сейчас, секундочку. Значит, это мы с вами уже проходили. Тут рассказывает, что такое Градилье. Сейчас это мы... Всё. Да, соответственно, логистическая регрессия, как она выглядела еще раз, что у нас есть тоже линейная комбинация, но мы берем сигнойду и, соответственно, дальше применяем такую Loss-функцию. В случае нейронной сети у нас Получается, что мы можем взять много нейронов. То есть если в логистической регрессии у нас было только одно место, куда стекались все... То есть я как бы тут надо слева, я хотел сравнение с логистической регрессией, как это выглядит. То есть представьте, что в логистической регрессии у нас получается, что по-прежнему вот красный входной сигнал, но он стекается в одно место, и у вас output layer. Получается, что логистической регрессии у вас не будет вот этого, и в фиолетовых будет только один прыжочек. То есть вот в этом разница, да, получается? А в нейронных сетях, в многослойных нейронных сетях, у вас может быть несколько слоев и много нейронных в каждом слое. Вот. Значит, то есть еще раз, да, давайте, знаете, как сделаем сейчас? Вот так вот сделаю. И получается, да, вот картинка внизу. Сейчас. Вот, видите, да, получается, что здесь все стекается в одно место, и там применяется симмоида. А здесь, видите, у нас получается много святий, много нейронов, много слоев. Ну, в данном случае два слоя. Вот в этом отличие. Значит, отличие. Так, продолжаем здесь. Да, может быть, на самом деле много выходов. Вот придумайте задачу, где может быть много выходов. Кто может предлагать? Есть решение? Несколько выходов, да, то же самое. У нас output var, Да, есть такой датасет, который называется ImageNet, там 1000 классов и 1000 выходных нейронов. Не нейронов, а выходов. Теперь давайте с вами рассмотрим прямое распространение. Сейчас нужно будет немножко напрячься, понимая, что конец дня и сложно, но давайте вот. Верхнеуровнево, давайте посмотрим, как это выглядит. То есть у нас, получается, есть... Ну, будем на таком участке нейронной сети смотреть, то есть мы сейчас не будем прям всю нейронку смотреть, а просто участок, да? То есть вот у нас есть, так вот, грубо говоря, Некий слой, мы его назвали ht-1, и ht — это выходной слой. И вот нам нужно посчитать, какие же будут выходы на основе значений, которые находятся здесь. И если абстрактно, то получается, что вот у нас есть вот эта самая функция активации, есть какие-то веса, v большое. Мы, значит, эти веса домножаем на значения из вот этого нейрона, из этого слоя, точнее, да, и прибавляем некий еще некий вид. Вот давайте подробнее рассмотрим, а что там лежит, потому что обратите внимание, что у нас здесь В большой, то есть это некая матрица с весами получается. И, собственно говоря, да, вот на всякие эти подписи не обращайте внимания. сравнение, что было и что стало. Было вот так вот, то есть это просто линейная модель. Линейная модель – это мы наши веса домножаем на признаки, то есть это получается матричное произведение в строке на столбец. А теперь обратите внимание, что у нас здесь происходит. Здесь у нас появляется матрица В. Откуда она появляется? Она как раз появляется оттуда, что у нас теперь тут много нейронов, да? И у каждого нейрона будут как бы свои связи вот с этим слоем. То есть, грубо говоря, вот здесь вот 1, 2, 3, 4, 5, 6. Вот, например, вот у этого нейрона будет V1, V2, V3, V4. Вот у этого нейрона будет V1, V2, V3, V4. И так вот шесть раз. И получается, вот если вы сюда так посмотрите, то здесь как раз оно и есть, что у вас как бы, ну вот тут в зависимости от того, как вы для себя решите, но давайте так, вот количество строк пусть будет, это количество нейронов, а количество столбцов, это пусть будет количество связей со следующим словом, да? То есть вот представьте, что V11, V12, V13, V14 — это будут веса, Вот эти вот, то есть V1-1, V1-2, V1-3, V1-4. И то же самое будет для этого нейрона, только у него будет V2-2, V2-3, V2-4 и так далее. То есть вот здесь уже видна разница между нейронкой и просто линейной моделью. Но как бы следует обратить внимание, что вот если вы каждую строку по отдельности будете умножать на эту штуку, у вас как бы по отдельности будет вот это. Просто много раз вы это делаете. То есть получается, что поэтому я как бы и начинал с линейных моделей, потому что на основе линейных моделей базируется, собственно, построение нейронных сутей. И, собственно говоря, здесь это и показано, что вот чтобы получить вот этот Синий квадратик, вам нужно строчку перемножить на столбец. А зеленая, да, это будет вот в эту строчку домножать на этот же столбец. Ну, на вектор, вот, с вечами. То есть через каждую сторону матрицы перемножаем? Да, то есть получается, что у вас в каждом... То есть получается, что вот у вас вот здесь, на самом деле, вот над каждой этой линией есть какой-то вес, по сути. И вы видите, что есть, получается, много. Поэтому и получается, что вам это удобнее в виде матриц выписывать. Вот почему я делал акцент на том, что в нейронках очень нужна матричная операция. Потому что вот такие операции сейчас, То есть в нейронках получается, что вот такие операции, они много-много раз выполняются для разных связей. И вы их можете сгруппировать с помощью матричных операций. Вот. Можно вопрос? Нейрон может передавать дальше, но всегда один и тот же вес передает, или он разный может передавать? Если деньги ставить под символы, то это, наверное, верхний нейрон слева. Он вправо и неровно разные веса передает? Да, да. Получается, на выходном слой у вас было четыре красных кружочка, в каждой кружочке будут другие веса разные. Вот. И эти веса будут подбираться в процессе обучения. Вот. Я сейчас вам покажу немножко другую иллюстрацию того, что я сейчас рассказал, немножко более сложную. Но как бы там прям подробно. Я поэтому призываю... Да-да, я знаю. Сейчас, пока он будет думать, я вам сейчас открою. Прямо... Так, сейчас. Пока, кстати, открываю, расскажите, вот как семинар прошел. Было клево? Я так посмотрел, вроде прикольно там все. Его сделал хорошее задание. Вот. Так, сейчас. Так, так, так, так, так. Окей. И, конечно, этот юмор. Так, ладно, я сейчас это всё вычисляю. Давайте пока запускайте, может, вопросы. Самое понятное, в чем фишка? Это просто число. То есть каждая нейрон, К нему на вход приходят много чисел, они как-то агрегируются, и через функцию активации уже выходит одно число. Я когда-то задавался, почему нужно эти слои, почему нельзя, типа, что сразу от тебя вход в новую нейронную дистрибьюцию? Ну, мы понимаем, что функция активации. Ну да, функция активация, то есть вот согласно той теореме, с которой я начинал, вам нужен хотя бы один слой, чтобы нелинейную зависимость аппроксимировать. А если бы вот то, что вы сейчас сказали, это, короче, вот это убрать и сразу сюда пустить сигнал. Это получается линейным? Да. Это много выходов. Много выходов? Ну, айл, пенсия, тренировка. А, это, кстати, уже другой материал немножко. Да, но, короче, я к чему. Вот здесь тоже показано, что вот у нас есть некий входной слой, скрытый выходной, и здесь показано, что вот здесь вот V1, V1, то есть получается V1 — это матрица 4х3. То есть, вот в квадратных стопках 1, это означает, что... Сейчас включу. Да, то есть, это означает на весь слой. На весь слой у вас веса будет матрица 4 на 3. Почему 4 на 3? Потому что 4 нейрона и 3 входных сигнала. То есть, у каждого нейрона получается по 3 веса, и таких нейронов 4, поэтому 4 на 3. И вот здесь подробно расписывается. Я просто быстро вам покажу, чтобы вы видели. Для качества нейрона получается вот такая вот запись. То есть опять два этапа. Линейная комбинация и нелинейность. И в итоге эта матрица выглядит вот таким образом. Ну, тут он просто не стал прямо расписывать, что три значения, да? Но суть в том, что каждая строка, это, короче, для каждого нейрона включается три веса, да? То есть здесь, получается, три столбца и четыре строки. И, получается, каждая строка — это веса соответствующего нейрона. Один, два, три, четыре. То есть у вас, получается, матрица весов в итоге в один большой, да? И обратите внимание, что на выходе из каждого нейрона свой сдвиг. Потому что, если вы помните линейную модель, там один сдвиг на всю модель. А так как у нас здесь много нейронов, и каждая нейрон — это модель сама по себе простая, то у каждого выхода свой сдвиг. Вот такие дела. Это я вам рассказал, как у нас сигнал слева направо распространяется. Итого, получается, что... Итого, получается, верно, возвращаемся к нашим слайдам. То есть вот эта матричная запись. Итого, у нас на самом деле вот то, что здесь изображено, это вот это. Давайте... Вот кто мне сможет сейчас указкой расшифровать, Что здесь происходит? Вот кто-то может подойти прямо и с указкой мне рассказать, что... что, ну, угадать, вот, то, что права в этой записи? Ну, или можете говорить, я буду... Ну да, а это, соответственно, да, это вот то, что здесь будет. Четыре числа будут. Ну, заточнее, стоп, не четыре числа, а сейчас... сейчас скажу, стоп, стоп. Получается... Ну, давайте я попробую. Да, давай. А сейчас... да, давай. Ну, здесь его 0.2, это вес первого картины, а f 0.2, это уже сам Первый нейрон. То есть вот нейрон делает эти функции, да? Да. Вот. Получается, V1 — это, скорее всего... Ну, V1, да. Почему V1? Ну, получается, на вход, наверное... А, V1 — это уже непонятно. Наверное, вес, вот этот нейрон добавляется в вес, то почему он V1? Понятно. Может быть, потому что он переходит в этот нейрон? Не, ну в плане, почему V1, это... Ну давайте, сейчас, так... Во-первых, x — это вот это желтое, да? Да, x — это все желтые значения. x — это x. Значит, дальше этот x с какими-то весами b0. Обратите внимание, b0++ означает, что в b0 застыты все веса, которые здесь, на этими линиями. И плюс некий b0 — тут тоже застыты все линии сразу. То есть эта матричная запись получается... И f0, да, это некая функция с активацией, применяется здесь. И получается, что на выходе вот из этого мы получаем значение для этого слоя, для слоя 1. Дальше, соответственно, мы используем значение этого слоя, чтобы получить значение для этого слоя. И поэтому получается, что для того, чтобы посчитать значение этого слоя, мы значение вот этого слоя теперь умножаем на матрицу V1. И получается, что V1 зашитывает все веса, которые здесь. И получается, к этому применению f1 получаем значение здесь. Теперь дальше, чтобы получить значение для этого слоя, мы значение, которое получили здесь, умножаем на V2. То есть V2 зашитывает все, что вот здесь. И получаем, соответственно, значение здесь. То есть это получается... А здесь в итоге мы получаем вот то, что есть. Ну и, наконец, мы домножаем на V3 выходы ревербераторного слоя и получаем конечное значение. То есть получается, что в V3 зашифрованы все вот эти нейроны, которые здесь. И получается в итоге, что если вот это все сделать, мы получаем выходные значения. Теперь обратите внимание, что это математическая запись. Я почему хотел подробно вам это рассказать, потому что получается, что, например, чтобы получить 2-2, вам нужны значения, когда... Ну, то есть, точнее, чтобы получить значения, например, скрытого слоя 2, вам нужны значения, скрытого слоя один, который, в свою очередь, получается из значений вторных. То есть это как такой матрешка. То есть вы как будто... Ну, чем дальше идем, тем более, получается, запись возрастается. Но в корне этой записи лежит простая вещь. Вы просто используете значение предыдущего слоя, а, в свою очередь, посчитайте этот слой, Используете его значение еще более предыдущего слоя. В итоге получается, что, чтобы получить значение на выходе, у вас используются все вот эти связи, все слои и так далее. То есть, как бы, вот эта запись — это вот это. Просто f — это, получается, уже функция активации? Ну да, везде это функция активации. Здесь это может быть на выходе. Я вам расскажу, что здесь может быть на выходе. То есть, на самом деле, если вот Если f0, f1, f2 это может быть ReLU, то для f3 там можно применить Softmax. Но только Softmax мы дальше используем. А на выходе мы пользуемся матрицей? На выходе получается... Вот это хороший вопрос, потому что... Четыре выхода. Да, четыре выхода. Можно конкретно, допустим, смотрите, сколько можно сделать. Можно... Сейчас. Я просто сразу пытаюсь вспомнить, как это реализовано в соответствующих библиотеках. И, по-моему, там... Да, там, по-моему, это взяло на выходе, да. То есть четыре значения, они дальше используются в любой расчётной локации. Ну, как бы, вы можете просто взять какое-то конкретное значение и соответственно... То есть, например, на выходе самое простое, что можно взять, там, посмотреть самый большой вес. И, значит, самое простое, что мы будем делать. Смотреть самый большой бейс, значит, мы там кошку обойти будем. Ну, смотрите, да. То, о чем ты говоришь, это только вы смотрите значение, которое здесь, и наибольшее значение, оно будет соответствовать классу, который предсказывает индексу класса, который предсказывает нейронку. Ну, это вы на семинаре увидите. Короче говоря, да, это правильно то, что вы говорите. Поэтому, опять же, в плане, что на выходе получится, зависит от того, что у вас будет в качестве v3 и так далее. То есть, если у v3 размерность будет на выходе 4, то вы получите 4 значения. Правильно, вообще-то? Ну да, да, правильно. Окей, и таким образом, вот, короче, давайте я опять разверну вот это. Будет своя матрица весов, своя... ну, функция активации, это зависит от того, как вы зададите. Обычно в рамках одного слоя задают одну и ту же функцию активации, но для разных слоев можно разные принципы. Не, ну можно, но обычно так не делают. Почему так не делают? Потому что вам нужно будет делать обратное распространение ошибки, и чем сложнее вы это будете задавать, тем сложнее будет считать эти градиенты. То есть обычно все, кто пытается в этом плане не слишком переусердствовать. То есть по-хорошему на картинке между слоями надо нарисовать еще полуксортивацию или нет? Ну да, тут и везде защита функции активации. На выходе из каждого слоя вот здесь, грубо говоря, есть нелинейность. То есть это матричная запись так называемого inference. Что такое inference? У вас есть на входе иксы, и вы их распространяете, чтобы получить выходные значения. То есть это, получается, распространение сигнала слева направо. Представьте, что вы уже нейронку обучили. Если на вход приходит х, чтобы получить выходные значения, вам нужно совершить, по сути, вот такие операции математические. Поэтому, по сути, работа нейронок – это просто перемножение матриц между собой с математической точки зрения. А теперь давайте, собственно, посмотрим, как их теперь обучать. Только вот мне, чтобы вам это рассказать. Да, вот сейчас получилось. Сейчас оно вернется. Если вопросы есть в чате, тоже задавайте. Сейчас я проверю, у нас всё идёт. Напишите там плюс всего нормального в чате. Да, всё, плюс. Спасибо. Вот, сейчас проект отключится, и я продолжу. Так. А теперь, собственно, мы с вами будем говорить про обучение. Напомните, как мы уже умеем обучать модели машинного обучения. Какой мы метод с вами рассмотрели. То есть, что нужно сделать, чтобы обучать модель машинного обучения. Come on, я в начале лекции говорю. Ну, если вы подумали, что самое важное... Ну, веса, да. Вот как веса пропустить? Ну, нужны данные и результаты ответов. Так, и дальше чем мы делаем? Вот у нас есть ответы, да, и есть выходы модели. Что с этим можно сказать? Понимаю. Читать ошибку. Читать ошибку. Дальше читаем внутри ошибки и минимизируем. Вот, а как минимизируем? Взяв принципы. Вот. Я это все почему? Потому что для нейронов остается все то же самое абсолютно. Уже знакомая вам формула. Мы для каждого веса, то есть представьте, у вас получается здесь куча-куча, вот над каждой линией у вас есть свой какой-то вес. И вот для каждого веса вам нужно совершить такую операцию. И как вы видите, чтобы ее совершить, вам нужно посчитать производную. Причем много производной. И как вы можете понять, чем дальше у вас идет вглубь нейронки, тем больше надо сделать вычислений. Это логично. Потому что, как я вам говорил, смотрите, можно вернуться в эту запись. Чтобы посчитать производную по V0, вам нужно... Видите, сколько здесь внешних оболочек? Получается 1, 2, 3. Как минимум, начать снаружи и внутрь вот так вот, короче, считать. Вот. И получается, что аналитически это делается не очень здорово. Особенно, когда у вас глубокая нейронная сеть, вот просто в лоб считать производные не очень здорово. Поэтому придумали такой лайфхак математический, я про него поговорил прямо на первой лекции, он лежит в основе нейронных сетей, и в частности вот Джеффри Хинтон, который получил Нобелевскую в этом году за физику, он этот метод разработал. Собственно, в чем заключается суть? Суть заключается в том, что мы сначала строим граф вычислений. Значит, это просто некая иллюстрация абстрактная, то есть Это не связано с этой картинкой, это просто некая математическая абстракция, чтобы вам объяснить, в чем заключается метод. Представьте, что вы хотите обновить вот эти веса. При этом вычислительный граф, представьте, что у вас какая-то абстрактная математическая функция. Чтобы получить выход, у вас есть две Короче говоря, сейчас давайте вот я дальше сейчас. Да, ну соответственно, то есть что здесь, блин, нарисовать? Ну ладно. То есть представьте, что получается y равно x1b11, и на это навешивается сигнал 1. В свою очередь сюда еще приходит сигнал отсюда, а сюда приходит сигнал и от x1, и от x2. И они все, короче, агрегируются здесь, и дальше мы получаем некий высот, да? Так вот, чтобы посчитать производные, можно воспользоваться лайфхаком. Значит, мы вводим такие, можно ввести такие промежуточные перемены, здесь, допустим, у1, у2, да? Чтобы причитать производную по U1, вам нужно отследить от выхода. Теперь мы двигаемся справа налево. Нам нужно отследить, каким путем нам нужно пройти, чтобы добраться сюда. Мы видим, что мы идем справа налево, идем-идем-идем, и вот по этому пути мы сюда набираемся. Теперь мы смотрим, какие у нас функции стоят у нас на пути. Мы видим, что у нас возникла сегмойда, значит, у нас будет производная DL по D сегмойда. Дальше мы берем производную сегмойда по Y, потому что мы уже здесь, и нам сюда нужно. получается dy по d sigma1. И, наконец, последняя производная получается d sigma1 по du1. И получается, чтобы посчитать вот эту производную... Обратите внимание, что наша задача — посчитать производную вот этой итоговой функции u1. И получается, что она, согласно методу взятия производных сложных функций, равна произведению всех этих производных. То есть получается, чтобы нам посчитать производную отсюда, вот выходной функции по какому-то переменной, нам нужно построить вычислительный граф, посмотреть путь распространения сигнала назад и взять все частные производные, которые у нас окажутся на пути, и перемножить их. То есть на самом деле можно было бы просто напрямую считать вот эту штуку. Но я потом приведу пример, то есть можно взять какую-то функцию, например, ту же Sigmoid и выбирать, например, Mono производный. Но это практика показывает, что когда у вас сложные функции, это сделать гораздо сложнее, чем если вы разбиваете на вот такие функции, потому что, как правило, вот такие производные по отдельности, они гораздо проще. Вот. И, собственно говоря, получается, что метод заключается в том, что вы берете выходную функцию и считаете вот эти все производные, которые у вас цитазываются на пути, и получается в итоге делать вот такие операции. То есть так вы можете понять. И мы хотели по v1 как-то считать. То есть по v1 у вас просто добавляется множитель du1 по dv1. Вот финальный пункт, вот здесь вот еще один множитель, да? И все, и получается dL по dV1 будет все то же самое, только вы здесь добавляете еще один множитель. Вот такой метод называется chain rule, или метод обратного распространения ошибки, да? То есть получается, что... Еще раз, как выглядит обучение нейронок, да? Получается, вы сначала... Вам нужно посчитать слоз функции, а чтобы посчитать слоз функции, вам нужно... распространить сигнал слева направо. И получается, что вы сначала высчитываете выходной сигнал, для этого слева направо двигаетесь. Вот вы, как только получили выходной сигнал, можете посчитать loss-функцию. И как только вы посчитали loss-функцию, вы теперь обратно двигаетесь. И берете все производные на пути и просто начинаете их перемножать. И как только у вас какой-то градиент появился, вы его применяете для метода... для этого, для градиентного спуска. То есть вы для каждого слоя можете делать свой градиентный спуск. Как только у вас появилось, да, какой-то градиент появился, вы сразу можете применить градиентный спуск. Собственно, получается вот в этом и... заключается метод, то есть он называется backpropagation. Почему backpropagation? То есть это буквально обратное распространение ошибки называется метод, потому что вы считаете ошибку, и вы буквально как бы справа налево теперь дальше занимаетесь такими вычислениями, то есть сигнал как будто справа налево обратно теперь идёт. Но не идёт как бы, а вы вот считаете все эти производы. Я теперь вам сейчас покажу, как это можно ещё рассмотреть сейчас вот в примере вот этого. Вот я еще раз повторяю, что вот в этой лекции там прям суперподробно, я как бы многие вещи сейчас опустил, но тем не менее. Так, так, так, сейчас. Здесь как бы немножко сложно, поэтому я не буду. Да. Forward and backward propagation. Да, вот смотрите, вот эта схемка. Давайте так. Зачем нам нужно слева направо распространять сигнал? Чтобы обучать нейронке, нам нужно сначала сделать forward, а потом backward. Вот здесь видно, зачем нам нужно делать forward. Дело в том, чтобы вообще Ну, помимо того, что нам нужно выходной сигнал досчитать, нам нужно... Ну, сейчас вернется картинка, я пока буду рассказывать. Нам нужно еще в процессе понять, а в каких местах нам считать производную. То есть, нам нужно все промежуточные значения, нам тоже нужны. То есть, давайте я попробую на примере экрана. То есть, грубо говоря, получается, вот здесь вот выходной сигнал, вот здесь вот. А вы можете здесь увидеть, здесь написано cache и все промежуточные значения. То есть мы на самом деле у нас, грубо говоря, мы еще параллельно храним все вот эти выходные значения слоев и так далее. Почему мы это делаем? Потому что они используются, когда мы будем делать обратное распространение ошибки. То есть получается, что в любом случае при облучении нейрона Вы что делаете? Вы много раз делаете одно и то же. Берете сигнал, распространяете слева направо, запомнили все промежуточные значения, посчитали loss-функцию и дальше начинаете считать производные в обратном порядке справа налево. И вот таким образом вы посчитали все градиенты, сделали градиентный спуск, теперь снова вы слева направо двигаетесь, считаете все значения, У вас нейронка же обновилась, когда вы веса поменяли. И вы с обновленными весами снова сделали forward propagation, backward propagation. Снова у вас веса поменялись. И вот так вот итеративно вы делаете до тех пор, пока функция ошибки не примет удовлетворительное значение для всей нейронки. Вот давайте вот. Понятно ли в целом механизм, как не вам пообучается? Давайте вот вопросы, потому что это довольно непросто. Какие у вас может вопрос? Так пока включит проекта и если в чате тоже есть вопросы, пишите. Вот, и сейчас я вам буквально пару моментов еще уточню. Давайте вот вопросы еще и... А зачем нам каждый раз каждым словом? А вот вы просто... Это, кстати, хорошее упражнение. Возьмите, нарисуйте какую-нибудь простенькую нейронку, например, в 5 парных сигналах, и промежуточный слой не делайте сильно большим, просто несколько нейронов, Допустим, да даже можно один слой промежуточный сделать и какой-то выходной слой. И попробуйте честно посчитать производную. Вот если вы будете делать по тому механизму, который я объяснил, по методу chain rule, то есть обратного распространения ошибки, и цепочкой считать производную, вы увидите, что по их считать вам нужно будет выйти дальше. Просто без них вы не сможете считать предельно. Поэтому вы как бы... заслабляется, грубо говоря, некий кэш или переиспользуется, когда считаются градиенты. Вот. И это как раз вот здесь видно. То есть, грубо говоря, мы справа налево, когда идем вот эти все там веса, какие-то сдвиги и выходные значения. А дальше мы все вот эти промежуточные значения сохраняем. И дальше переиспользуем, когда считаем градиенты обратно. Здесь уже как бы возникает DA — это уже градиент, да? И вот. И считаете градиенты, и на основе всего этого получаются градиенты по параметрам. И как только вы получили градиенты по параметрам, делайте обратное распространение ошибки. И так вот много-много раз вы делаете, пока вам этого не хватит. Вот. Теперь возникает вопрос. Вопрос следующий сейчас. Кстати, обратите внимание, что здесь еще функция активации на весь слой, а здесь сегмойд — это уже выходной слой. Там, допустим, если у вас бинарная классификация, делаете сегмойду и получаете выходное значение от 0 до 1. Дальше читаете лозунг функцию, дальше отсылка к опалуз функции. Короче говоря, дальше Вот то, что я рассказал. Вот. А теперь возникает вот вопрос. Да, вот здесь, кстати говоря, отвечаю вот на ваш вопрос, да. Обратите внимание, вот что вы считаете, допустим, какой-то, то есть здесь приведена формула в целом, да, для какой-то там нейронной, ну, для вот тех нейронных сетей, которые мы рассматривали, при этом функция активации здесь бинарная, Binary LogLoss. И, короче говоря, здесь вы видите, что, например, чтобы посчитать производную d, v, l, здесь используется значение a, l. И получается, мы получаем как раз, когда идем слева направо. Но я просто не стал вводить эту формулу, потому что это То есть их можно без проблем вывести, я просто вам целый механизм объяснил. Но проследить, как это происходит, полезно. Обратите внимание, что здесь везде матричная операция, потому что у вас там много весов, много параметров, и для каждого нейрона это расписывать не имеет смысла. Поэтому матричная операция. То есть, еще раз, получается, слева — это forward, справа — это backward. Хорошо. Так, а теперь я вот что хотел вам еще рассказать. Сейчас скажу. Сейчас, помни, секундочку. Так. Сейчас, надеюсь, это было... Сейчас, ребята, я найду это место. Где-то я его только что видел. Вот он. Теперь вопрос. Вот, собственно, вот с этим вопросом, как веса-то инициализировать, да, потому что у вас должно быть некое начальное значение весов, оно же не просто у вас какое-то определенное, то есть как вообще, еще раз, да, вот у вас есть нейронка, изначально она не облачена, это означает, что у нее есть какие-то веса, но эти веса, вы их должны как-то задать, значение этих весов, да, и получается, что есть некие эвристики, как это лучше сделать, то есть Я вам сейчас расскажу, как лучше не делать. Вот, можно просто взять и все задать нулями. Вот здесь на примере вот совсем простейшей нейронки показывается, что так делать не очень хорошо, потому что у вас там на выходе получаются вот такие вот какие-то симметричные градиенты, и, короче говоря, ну тут просто, короче, И берется, да, напрямую вот считается, дальше берутся градиенты, и покажется, что градиенты у вас будут вот такого вида. То есть такие симметричные градиенты, и это не очень хорошо с точки зрения обучения получается. Помимо этого, если вы зададите нулями, да, все, у вас может возникнуть риск того, что выходные значения будут принимать маленькие значения, и у вас может возникнуть вот эта проблема затухания градиента. Кстати, откуда возникает проблема затухания градиента? Вот это видно, кстати, вот здесь. Она как раз из-за этого и возникает, что если каждый из этих градиентов маленький, то вот это произведение будет еще меньше. Получается, что места между слоями не меняются. Не очень понял. Когда нажимаешь несколько слоев, Или мы сразу задаем? Они меняются. В том-то и дело, что если у вас градиенты нулевые, близкие к нулю, то они не будут обновляться. А если градиенты... Ну да, здесь еще это градиенты. Если вот тачный градиент, он принимает небольшое значение, то все это произведение, тем более, будет маленькое. И получается, чем глубже вы будете идти внутрь нейронной сети, тем меньше у вас будет обновления весов. Поэтому для глубоких нейронных сетей есть риск того, что обучение для глубоких слоев будет проходить сложуче. для выходных, вот. И тут как раз поэтому нулями задавать не очень хорошо. То есть, я вам дальше расскажу про евристике, но можно, например, вот сделать таким образом, то есть, грубо говоря, задать значения не очень большими, не очень маленькими, а как бы что-то такое между, да, грубо говоря. И, то есть, можно, допустим, взять какой-то равномерное распределение случайное, но одноножить на номер один, чтобы оно не было слишком большое. Ну, короче, это такой момент немного эвристический, то есть тут есть некие подходы, но я про них чуть дальше расскажу. Просто тут в целом я... А вы же слизалась к родному по своему, так? Ну, как бы здесь да, но вообще есть некие эвристики, как лучше это сделать. И я про это расскажу на следующем занятии, если не забуду. специальные методы, я вам потом ссылку пришлю, как они называются и как выглядят. Там немножко более сложная игристика, но в целом есть подходы, как лучше сделать. То есть, если взять тот же PyTorch, вы когда там программируете нейронную сеть, там обычно уже на автомате с какими-то весами задается, с теми самыми евристиками, о которых я сейчас говорю. Не с этими, а с которыми есть подходы. Просто я про них чуть дальше расскажу. Это первый момент, что у вас, получается, результат будет сильно зависеть от того, с каким начальным приближением вы начнете обучать нейронку. Вот, и что еще? И тут про что рассказывают? Тут рассказывают про то, что у вас вот когда много слоев и, допустим, у вас задача там какая-то на компьютер визент, то тут глубокая representation. То есть, грубо говоря, в глубоких слоях у вас будут более простые элементы фичи. То есть на выходах из нейронов в более глубоких слоях у вас выходы будут создавать какие-то совсем простые фичи. И чем ближе к выходу, тем более у вас тем больше вас будут напоминать уже что-то более осмысленное. Это возникает из-за того. Из-за того, что вы обучаетесь, да, и вот так получается. Есть такая некая штука с нейронами, что в глубоких слоях у вас более простые фичи задаются, а дальше оно Чем ближе к выходу, тем у вас более широкая смысленность. Это если не интуитивным образом рассказывать. Есть целая работа, которая изучает как слоение нейронных сетей, есть ли в них какой-то смысл, пытаются изучать как слои, что-то физическое, и так далее. Особенно вот я видел работы, связанные с LLM, но фишка в том, что чем больше у вас нейронка, тем сложнее уже анализировать, потому что у нее куча слоев, куча параметров. Но некая интуиция вот в этом заключается. Что еще вам? рассказать. В принципе, вроде я рассказал, как обучаются нейронки в целом. Получается, здание именно из-за этого архитектуры? Ну, не прямо из-за этого архитектуры, а в целом. В целом, да. Ну, смотрите, я чуть подробнее расскажу. То есть там оба этих человека, с которым дали, да, там один Кинтон, а другой Хопкинс, да, по-моему. И оба они стояли у истоков, то есть один из них как раз предложил метод обратного распространения ошибки. Но обоим дали как бы формально, грубо говоря, им дали за то, что они придумали всякие архитектурки, причем придумали их на основе неких физических инсайдов, то есть на основе как бы физических, как это сказать, ну то есть у них в голове крутилась некая физическая модель, они ее переложили в виде математики и получили какие-то там модель Больсмана еще, вторая какая-то модель, а, ну и модель Копкинса, вот. То есть они ее получили на основе инсайтов из физики. В итоге это стало существенным вкладом в нейронки, то есть вот эти все вещи. И поэтому, ну, почему именно за физику дали, непонятно, но вот считается, что в формальном релизе написано, что они на основе физики вывели свои вот эти нейронки. Но сами нейронки стали активно применяться не только физике, а в разных областях. В общем, Вот если вкратце, то получается такая история. Что я хотел сказать? Давайте последний момент скажу. Если вы захотите запрограммировать это своими руками, Понятно, что сейчас есть фреймворки, которые это все делают. Тот же PyTorch автоматически умеет строить вот эти самые вычислительные графы и высчитывать производные. То есть, как это делать дальше, вот Егор на семинарах расскажет. Но, допустим, вы хотите вот это упражнение сделать это сами. Чтобы сделать это сами, вам нужно будет применить один лайфхак, про который я сейчас расскажу. Сейчас, буквально, одну секунду. Лайфхак вот в этом заключается. Сейчас. А, запустилось, да? То есть лайфхак заключается в том, что как можно меньше использовать циклы всякие, потому что это сильно замедляет, во-первых, во-вторых, усложняет. На примере вот таких простых вещей, что… Лямбда функции? Не, не лямбда функции, а просто… Вот простой пример, да, там было, например, yx плюс b, и b – это столбик с числами, но все числа одни и те же, например, для линейной регрессии. У вас получается, что вот если b – это 100, а вам нужно 4b, вы можете оставить это просто стол, он автоматически, ну, в NumPy, например, он автоматически из этого сделан. Стол вы перепиливаете, и тогда у вас операция как бы получится. То есть, грубо говоря, поэтому получается и здесь дальше вот такая штука, что когда речь идет о всяких операциях сложения, учитания и так далее, или умножения на число какое-то, то не надо сгородить как бы цикл, в том же NumPy это все делается, по автоматически это все делается. Или, например, в SigMod, там экспоненты, применяете экспоненты от минус x на v, то есть вы можете просто сделать np.s и дальше туда сразу матрицу задать. Он для каждого элемента матрицы применит экспоненты. То есть вам не нужно писать цикл for и в этом цикле писать для каждого элемента экспоненты. Ну, короче, это просто некие такие лайфхаки. Растить себе жизнь, если вы захотите это делать. Так, ну что, ребят, мы уже, да, уже больше полу... А, ну мы как раз, кстати, к девяти подходим. В принципе, на самом деле, у меня особо добавить в ходе нечего такого. То есть я призываю посмотреть материалы, посмотреть отдельно вот лекции. Ну и давайте закончим тем, что откуда вообще взялась вот эта тема, что нейронная сеть, да? Типа вот человеческий нейрон, Там тоже, получается, есть некое ядро, в это ядро приходят сигналы. То есть каждый нейрон связан с другим нейроном, и они связаны вот через такие вот штуки. И приходят сигналы, они в ядре как-то обрабатываются и уходят дальше. Но это просто какое-то выходное значение. Но суть в том, что надо понимать, что вот это и вот это, это, конечно, не прямая аналогия, потому что, мне кажется, нейробиологи до сих пор не очень понимают, что здесь происходит в ядре реального нейрона человеческого. Но это чисто вот такая математическая абстракция, и просто из-за того, как это похоже выглядит, вот эта картинка, не надо думать, что это прям Реальная модель. Вот эта модель. То есть это просто некая математическая абстракция. Просто я рассматриваю нейронные сети, потому что тут тоже, кажется, вот этот нейрон связан с предыдущим. Тоже идут какие-то сигналы, какие-то нелинейности. Надо понимать, Вот нейронные линейности, которые в этой модели, они, конечно же, гораздо проще, чем то, что реально происходит. Но это просто некая математическая абстракция, поэтому назвали нейронной сетью. Короче, из этой лекции вам нужно вынести еще раз, по сути, вынести две вещи. То есть, как происходит вот эта вещь. Точнее, три вещи. Во-первых, как выглядит устройство отдельно нейрона. То есть, еще раз, выглядит это таким образом. Дальше вам нужно вынести, как выглядит распространение сигнала слева-направо. И, наконец, вам нужно разобраться, как работает механизм обратного распространения ошибки или Chain Rule. Если вы захотите подробно смотреть те формулы, как это выглядит, то можете смотреть вот эту PDF, там немножко непонятно, но Можно найти в ютубе видеозапись этой лекции, и там, соответственно, это сопровождает, то есть можно проследить. Мне лично, для понимания, очень помогла эта лекция. Это вот из курса специализации Deep Learning. Это диплёрник, можете прям загуглить в ютубе. Ну короче, пока он немножко работает, можете в ютубе пить и прям эти лекции найти. Там прям мужик, что мне лично очень понравилось, что он прямо супер подробно выводит вот эти вещи, сейчас покажу какие. То есть он начинает с логистической регрессии. А вот тут Эндрю Анджи. Да-да-да, это Эндрю Анджи. Он начинает с логистической регрессии и показывает дальше переход в нейронки. Особенно здесь он прямо для каждого нейрона расписывает эти уравнения. Дальше показывает, как это все векторизуется. Причем это показывает векторизацию, дальше идет. То есть он показывает векторизацию на весь датасет. А я вам показал, по сути, только для какого-то Друго говоря, я вам показал для одной строчки из датасета. То есть я вам, по сути, показал векторизацию для одного элемента из датасета, а можно вот эти все вот вещи для всех элементов из датасета обобщить. Ну, это сложно здесь разобраться, поэтому не стал рассказывать, но это тоже можно сделать. И поэтому... И вот здесь он подробно про это говорит. Короче говоря, вот если вам это будет интересно, посмотрите, потому что вы прям реально разберетесь механизм работы нейронных детей. Вы посмотрев эту лекцию, разберетесь. Ну а, соответственно, Егор вам будет дальше рассказывать, как это программировать, обучать, собственно. И вот так и так. Вроде я все развернул, если что. Так, а ну-ка, Teams, если есть вопросы, давайте. Потому что, в принципе, я рассказал то, что хотел с вами. Напишите плюс, если все было понятно. Хорошо. Ну что, если кто хотел со мной пообщаться, то, пожалуйста, приходите. Спасибо. До свидания.
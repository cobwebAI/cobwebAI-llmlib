# Лекция по машинному обучению: Нейронные сети

## Введение
- Нейронные сети — ключевая тема в машинном обучении.
- Рассмотрим архитектуру нейронных сетей, начиная с перцептрона и переходя к многослойным нейронным сетям.
  
## Основные концепции

### 1. Постановка задач машинного обучения
- Данные $X$ и ответы к ним $Y$. Например, в случае с квартирами:
  - Каждый объект (квартира) имеет признаки $X_1, X_2, ...$.
  - Необходимо предсказать цену (ответ $Y$).
- Задача — найти алгоритм $A$, который преобразует $X$ в $Y$.
- Вводится функция потерь, отображающая расстояние между предсказанными и истинными значениями.

### 2. Линейные модели
- Формулы для линейных моделей представлены в матричном виде.
- Функции потерь: MSE (среднеквадратичная ошибка) и MAE (абсолютная ошибка).
- Используем градиентный спуск для минимизации функции потерь.

### 3. Классификация
- Задача классификации (например, бинарная) требует выделения разделяющей прямой.
- Сигмоидная функция используется для оформления линейной комбинации, которая определяет вероятность класса.

## История нейронных сетей
- Первые модели появились в 20 веке, но их применение было ограничено вычислительными мощностями.
- В 90-х годах возникли проблемы с хранением данных, однако с развитием видеокарт с 2000-х нейронные сети начали активно применяться.
- AlexNet продемонстрировала превосходство нейронных сетей над традиционными методами компьютерного зрения.

## Преимущества нейронных сетей
- Нейронные сети продолжают улучшать качество предсказаний с увеличением объема данных.
- Классические алгоритмы машинного обучения достигают плато, в то время как нейронные сети демонстрируют рост качества.

## Архитектура нейронных сетей
- Нейрон получает входные значения, которые агрегируются и передаются через функцию активации, производя одно выходное значение.
- Без функции активации модель сводится к линейным зависимостям.
- Оптимизация весов нейронов — важный шаг обучения нейронной сети.

### Иерархия признаков
- Нейронные сети выявляют иерархическую структуру признаков:
  - На низких уровнях: простые формы (к_edges, углы).
  - На высоких уровнях: более абстрактные структуры (лица, машины).
  
## Обучение нейронной сети
- Обновление весов происходит через градиентный спуск на основе ошибок выходов.
- Разнообразные оптимизационные алгоритмы (Adam, RMSprop) используются для улучшения сходимости.

## Реализация нейронных сетей
- Используются фреймворки, такие как PyTorch, для автоматизации расчета производных и построения вычислительных графов.
- Рекомендации по оптимизации: избегать вложенных циклов, использовать векторизированные операции (например, NumPy).

## Заключение
- Нейронные сети — это мощный инструмент для моделирования сложных зависимостей.
- Основные моменты:
  - Понимание механизма работы нейронов.
  - Применение концепции обратного распространения ошибки.
- Рекомендуется изучить дополнительные ресурсы для углубления знаний (например, лекции Андрю Нг).

## Обсуждение
- Вопросы и уточнения по материалу лекции приветствуются.
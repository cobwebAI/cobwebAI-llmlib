# Лекция по машинному обучению: Нейронные сети

## Введение
- Обсуждаем нейронные сети и их архитектуру.
- Начнем с модели перцептрона, на основе которой будут строиться многослойные нейронные сети.

## Постановка задач машинного обучения с учителем
- Данные: $X$ — признаки (например, характеристики квартир) и $Y$ — ответы (например, цены).
- Задача: предсказать $Y$ на основе $X$.
- Необходимо найти алгоритм $A$, который отображает $X$ в $Y$.

## Функция потерь
- Функция потерь (ошибки) показывает, насколько близко предсказание к настоящему ответу.
- Основная задача — минимизация этой функции потерь, изменяя параметры модели.

## Линейные модели
1. **Формула линейной модели**: $Y = X \cdot W + b$, где:
   - $X$ — матрица признаков.
   - $W$ — вектор весов.
   - $b$ — смещение.
2. **Метрики для регрессии**:
   - Среднеквадратичная ошибка (MSE): 
     $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$
   - Средняя абсолютная ошибка (MAE): 
     $$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$

3. Используем **градиентный спуск** для оптимизации весов.

## Классификация и логистическая регрессия
- Задача классификации: отделение объектов на два класса.
- Используем **сигмоидную функцию**: 
  $$ \sigma(Z) = \frac{1}{1 + e^{-Z}} $$, где $Z$ — линейная комбинация входов.
- Предсказания $Y$ принимают значения 0 или 1.

## История нейронных сетей
- Первые модели нейронных сетей появились в 20 веке, но их использование ограничивалось теоретическими исследованиями.
- Появление видеокарт в 2000-х годах привело к росту популярности нейронных сетей, особенно в области распознавания образов (пример: AlexNet).

## Преимущества нейронных сетей
- Нейронные сети могут эффективно моделировать сложные, нелинейные зависимости, что делает их более гибкими по сравнению с линейными моделями.
- Связывается с **иерархией признаков**: низшие уровни моделей ищут простые зависимости, а высшие уровни — более сложные.

## Обучение нейронной сети
- Основная цель: оптимизация весов через градиентный спуск.
- Важность инициализации весов: неправильная инициализация может затормозить обучение.
- Используем различные оптимизационные алгоритмы (например, Adam).

## Архитектура нейронных сетей
- Нейрон как единичная единица: принимает множество входов, обрабатывает и передаёт выходное значение через функцию активации.
- Функции активации помогают моделировать нелинейные зависимости.

## Исследования и разработчики
- Джеффри Хинтон и Юша Бенжио стояли у истоков нейронных сетей и разработали метод обратного распространения ошибки.

## Рекомендации по программированию
- Использование библиотек, таких как **PyTorch** и **NumPy**, для работы с большими объемами данных и вычислений.
- Оптимизируйте код, минимизируя количество циклов для повышения эффективности.

## Заключение
- Нейронные сети представляют собой мощный инструмент в машинообучении, способный решать сложные задачи.
- Рекомендуются дополнительные материалы и лекции для более глубокого изучения.

## Вопросы
- Участники могут задавать вопросы по материалу лекции или уточнять неясные моменты.
Но я про них чуть дальше расскажу, просто тут в целом я… А вы же занимаетесь аранномно присваивать? Аранномно присваивать? Ну, как бы здесь да, но вообще есть некие эвристики, как лучше это сделать. И я про это расскажу на следующем занятии, если не забуду. Там есть специальные методы, я вам потом ссылку пришлю, как они называются и как выглядят. Там немножко более сложные эвристики, но в целом есть подходы, как лучше сделать. То есть если взять тот же PyTorch, вы когда там программируете нейронную сеть, там обычно уже на автомате с какими-то весами задается, с теми самыми эвристиками, которые я сейчас говорю, не с этими, а которые есть подходы, просто я про них чуть дальше расскажу. Это первый момент, что у вас результат будет сильно зависеть от того, с каким начальным приближением вы начнете обучать нейронку. И что еще? Тут про что рассказывают? Тут рассказывают про то, что у вас когда много слоев и, допустим, у вас задача какая-то на компьютер визент, то тут глубокая representation. То есть, грубо говоря, в глубоких слоях у вас будут более простые элементы фичи. То есть на выходах из нейрона в более глубоких слоях у вас будут создаваться какие-то совсем простые фичи. И чем ближе к выходу, тем больше у вас выходов будут напоминать уже что-то более осмысленное. Это возникает из-за того, что вы обучаете, и так получается. Есть такая некая штука с нейронкой, что в глубоких слоях у вас более простые фичи задаются, и дальше чем ближе к выходу, тем у вас более что-то осмысленное. Это если не интуитивным образом рассказывать. Есть целая работа, которая изучает как слои нейронных сетей, есть ли в них какой-то смысл. Пытаются изучать как слои что-то физическое создают и так далее. Я видел работу, связанную с LLM. Но фишка в том, что чем больше у вас нейронка, тем сложнее анализировать, потому что у нее куча слоев, куча параметров. Но некая интуиция в этом заключается. Что еще вам рассказать. Вроде я все рассказал. Как обучаются нейронки в целом. Вы сейчас дали именно за эту архитектуру анализ? Не прямо за эту архитектуру, а в целом. За эти линейки? В целом, да. Смотрите, я чуть подробнее расскажу. Там оба этих человека, с которым дали, один Кинтон, а другой Фопкинс. И оба они стояли у истоков. Один из них как раз предложил метод обратного распространения ошибки. Но обоим дали формально за то, что они придумали всякие архитектурки. Причем придумали их на основе физических инсайтов. У них в голове крутилась некая физическая модель. Они ее переложили в виде математики и получили модель Больсмана, и модель Фопкинса. Они ее получили на основе инсайтов из физики. В итоге это стало существенным вкладом в нейронки. Почему именно за физику дали, непонятно. Но считается, что в формальном релизе написано, что они на основе физики вывели свои нейронки. Но сами нейронки стали активно применяться не только в физике, а в разных областях. В общем, если вкратце, то получается такая история. Что я хотел сказать. Давайте последний момент скажу. Если вы захотите запрограммировать это своими руками, понятно, что сейчас есть фреймворки, которые это все делают. Тот же PyTorch автоматически умеет строить самые вычислительные графы и высчитывать производные. Как это делать дальше, Егор на семинарах расскажет. Но, допустим, вы хотите сделать это сами. Чтобы сделать это сами, вам нужно будет применить один лайфхак, про который я сейчас расскажу. Лайфхак заключается в том, что как можно меньше использовать циклы. Потому что это сильно замедляет, во-первых, во-вторых, усложняет. На примере таких простых вещей. Лямбда функции? Не лямбда функции, а простой пример. Было, например, ax плюс b, и b это столбик с числами. Но все числа одни и те же, например, для линейной регрессии. У вас получается, что если b это столбик, а вам нужно 4b, вы можете оставить это просто столбик. Нумбай, например, автоматически из этого сделает столбик. И тогда у вас операция получится. Грубо говоря, получается и здесь дальше такая штука, что когда речь идет о всяких операциях сложения, учитания и так далее, или умножение на число какое-то, то не надо сгородить циклы. В том же Нумбай это все делается автоматически. Или, например, в Sigmoid вы меняете экспоненты от минус x на y. То есть вы можете просто сделать np.x и туда сразу матрицу задать. Он для каждого элемента матрицы применит экспоненты. То есть вам не нужно писать цикл for и в этом цикле писать для каждого элемента экспоненты. Ну, короче, это просто некие лайфхаки. Растить себе жизнь, если вы захотите это делать. Мы как раз к 9 подходим. В принципе, на самом деле у меня особо добавить нечего. То есть я призываю посмотреть материалы, посмотреть отдельно лекции. Но давайте закончим тем, что откуда вообще взялась вот эта тема, что нейронная цель. Типа человеческий нейрон. Там есть некое ядро, в это ядро приходят сигналы. То есть каждый нейрон связан с другим нейроном. И они связаны через такие вот штуки. И приходят сигналы, они в ядре как-то там обрабатываются и уходят дальше. Но это просто какое-то выходное значение. Но суть как бы в том, что надо понимать, что вот это и вот это, это, конечно, не прямая аналогия. Мне кажется, нейробиологи до сих пор может не очень понимать, что вот здесь происходит в ядре реального нейрона человеческого. Но это чисто вот такая математическая абстракция. И просто из-за того, как это похоже выглядит, вот эта картинка, не надо думать, что это прям реальная модель. Вот эта модель. То есть это просто некая математическая абстракция, просто ее назвали нейронной сетью. Потому что тут тоже кажется, что вот этот нейрон связан с предыдущим. Тоже идут какие-то сигналы, какие-то нелинейности. Но надо понимать, что нелинейности, которые в этой модели, они, конечно, уже гораздо проще, чем то, что реально происходит. Это просто некая математическая абстракция, поэтому назвали нейронной сетью. Из этой лекции вам нужно вынести еще раз, по сути, две вещи. Как происходит вот эта вещь. Точнее, три вещи. Во-первых, как выглядит устройство отдельно нейрона. Еще раз. Выглядит это таким образом. Дальше вам нужно вынести, как выглядит распространение сигнала слева направо. И, наконец, вам нужно разобраться, как работает механизм обратного распространения шелки или chain rule. Если вы захотите подробно смотреть те формулы, как это выглядит, то можете смотреть вот эту PDF. Там немножко непонятно, но можно найти в YouTube видеозапись этой лекции. Там, соответственно, это сопровождает. Можно проследить. Мне лично, для понимания, очень помогла эта лекция. Это из курса специализации Deep Learning. Это Deep Learning. Можете прямо загуглить в YouTube. Пока он немножко работает, можете в YouTube пить и прямо эти лекции найти. Что мне лично очень понравилось, что он прямо суперподробно выводит вот эти вещи. Он начинает с логистической регрессии. Это Андрю Анджи. Да, это Андрю Анджи. Он начинает с логистической регрессии и показывает переход к нейронке. Особенно здесь он прямо для каждого нейрона расписывает эти уравнения. Дальше показывает, как это все векторизуется. Причем это показывает векторизацию. Он показывает векторизацию на весь датасет. А я вам показал для одной строчки из датасета. Я вам, по сути, показал векторизацию для одного элемента из датасета. Можно вот эти все вещи для всех элементов из датасета обобщить. Я просто сложно здесь разобраться, поэтому не стал рассказывать, но это тоже можно сделать. И вот здесь он подробно про это говорит. Короче говоря, если вам это будет интересно, посмотрите, потому что вы прям реально разберетесь. Механизм работы нейронных детей вы, посмотрев эту лекцию, разберетесь. Ну а, соответственно, Егор вам будет дальше рассказывать, как это программировать, обучать. И вот так. Вроде я все разобрал. Если что. А ну-ка, Teams, если есть вопросы, давайте. Потому что, в принципе, я рассказал то, что хотел с вами. Общайте плюсы, если все было понятно. Если кто хотел со мной пообщаться, то, пожалуйста, приходите. Спасибо.
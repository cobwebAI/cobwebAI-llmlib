ht-1 и ht — это выходной слой. И вот нам нужно посчитать, какие же будут выходы на основе значений, которые находятся здесь. И если абстрактно, то получается, что вот у нас есть вот эта самая функция активации, есть какие-то веса, V большое. Мы, значит, эти веса домножаем на значения из вот этого нейрона, из этого слоя, точнее, и прибавляем еще некий V. Давайте теперь подробнее рассмотрим, а что там лежит. Потому что обратите внимание, что у нас здесь V большое. То есть это некая матрица с весами получается. И, собственно говоря, на всякие эти подписи не обращайте внимания, а сравнение, что было и что стало. То есть было вот так вот. То есть это просто линейная модель. Линейная модель — это мы наши веса домножаем на признаки. То есть это получается матричное произведение в строке на столбец. А теперь обратите внимание, что у нас здесь происходит. Здесь у нас появляется матрица V. Откуда она появляется? Она как раз появляется оттуда, что у нас теперь тут много нейронов, и у каждого нейрона будут свои связи с этим слоем. То есть, грубо говоря, вот здесь вот раз, два, три, четыре, пять, шесть. Вот, например, у этого нейрона будет V1, V2, V3, V4. У этого нейрона будет V1, V2, V3, V4. И так вот шесть раз. И получается, если вы сюда посмотрите, то здесь как раз оно и есть. Ну, тут зависимость от того, как вы для себя решите, но давайте так. Количество строк пусть будет количество нейронов, а количество столбцов — это пусть будет количество связей со следующим слоем. То есть, вот представьте, что V1,V1,V1,2,V1,3,V1,4 — это будут веса вот эти вот. То есть V1,V1,V1,2,V1,3,V1,4. И то же самое будет для этого нейрона, только у него будет V2, V2, V3, V4 и так далее. То есть вот здесь уже видна разница между нейронкой и просто линейной моделью. Но как бы следует обратить внимание, что если вы каждую строку по отдельности будете умножать на эту штуку, у вас по отдельности будет вот это, просто много раз вы это делаете. То есть получается, что… Поэтому я как бы и начинал с линейных моделей, потому что на основе линейных моделей базируется построение нейронных сутей. И, собственно говоря, здесь это и показано, что чтобы получить вот этот синий квадратик, вам нужно строчку перемножить на столбец. А зеленая, да, это будет вот эту строчку умножать на этот же столбец. Ну, на вектор с вечами. То есть за каждую строку мы умножаем в атмосфере? Да, то есть получается, что у вас вот здесь на самом деле над каждой этой линией есть какой-то вес, по сути. И вы видите, что их получается много, поэтому и получается, что вам это удобнее в виде матриц выписывать. Вот почему я делал акцент на том, что в нейронках очень нужна матричная операция, потому что вот такие операции, они много-много раз выполняются для разных связей. И вы их можете сгруппировать с помощью матричных операций. Нейрон может передавать дальше, но всегда один и тот же вес передает, или он разный может передавать? Если один... Там ставят симпатику. Верхний нейрон слева, он правый нейрон и разные веса передает? Да, да. Получается, на выходном слой у вас было четыре красных кружочка, в каждой кружочке будут другие веса разные. И эти веса будут подбираться в процессе обучения. Я сейчас вам покажу немножко другую иллюстрацию того, что я сейчас рассказал, немножко более сложную, но как бы там прям подробно. Я поэтому призываю... Уже опять про это все. Да, да, я знаю. Сейчас пока он будет думать, я вам сейчас открою. Пока, кстати, открываю. Расскажите, вот как семинар прошел? Клево было? Я так посмотрел, вроде прикольно там все. Егор сделал хорошее задание. Так, ну ладно, я сейчас приду в следующий раз. Давайте пока запускайте, может, вопросы. Всем вам понятно, в чем фишка? В том виде, да, здесь есть слаймер. Это просто число. То есть каждый нейрон, к нему на вход приходит много чисел, они как-то агрегируются, и через функцию активации уже выходит одно число. У тебя когда-то задался вопросом, почему нужно эти слои, почему нельзя создать обходного бронированного эффекта. Но мы хорошо понимаем, что функция активации. Это, кстати, уже другой материал немножко, но, короче, я к чему. Вот здесь тоже показано, что у нас есть некий входной слой, скрытый выходной. И здесь показано, что вот здесь вот В1, В1. То есть получается В1 это матрица 4 на 3. То есть вот в квадратных скобках 1 это означает, что на весь слой веса будет матрица 4 на 3. Почему 4 на 3? Потому что 4 нейрона и 3 входных сигнала. То есть у каждого нейрона получается по 3 веса, и таких нейронов 4. Поэтому 4 на 3. И вот здесь подробно расписывается, я просто быстро вам покажу, чтобы вы видели. То есть для каждого нейрона получается вот такая вот запись. То есть опять два этапа. Линейная комбинация и не линейная. И в итоге эта матрица выглядит вот таким образом. Тут он просто не стал расписывать, что 3 значения. Но суть в том, что каждая строка для каждого нейрона отличается при веса. То есть здесь получается 3 столбца и 4 строки. И получается каждая строка это веса соответствующего нейрона. 1, 2, 3, 4. То есть у вас получается матрица весов. В итоге вы один большой. И обратите внимание, что на выходе из каждого нейрона свой сдвиг. Потому что если вы помните линейную модель, там один сдвиг на всю модель. А так как у нас здесь много нейронов и каждая нейрона это модель сама по себе простая, то у каждого выхода еще свой сдвиг. Вот такие дела. Это я вам рассказал, как у нас сигнал слева направо распространяется. Итак, возвращаемся к нашим слайдам. Вот эта матричная запись. Итого у нас на самом деле вот то, что здесь изображено. Это вот это. Кто мне сможет сейчас указкой расшифровать, что здесь происходит. Кто-то может подойти и с указкой мне рассказать, что угадать, расшифровать эту запись. Ну или можете говорить, я буду... Ну да, а это соответственно да. Это вот то, что здесь будет. Четыре числа будут. Ну, заточнее, стоп, не четыре числа. Сейчас скажу стоп, стоп. Получается... Давайте я попробую. Да, давай. Твоя функция, по сути, активировалась. А сейчас... да, давай. Ну, здесь вот 0, это вес первого картины, а f 0, это уже сам первый нейрон. Ну вот, я и сниму. А, да. То есть вот этот нейрон делает эту функцию. Так. Вот. Получается, v1, это скорее всего... Ну, v1, да. А почему v1? Ну, получается, на вход, наверное... А, хм. А, понятно. Наверное, вес, вот этот нейрон добавляет вес, но почему он v1? Понятно. Может быть, потому что он переходит в этот нейрон? Не, ну, в плане, почему v1, это... Ну, давайте сейчас, во-первых... Во-первых, х это вот это желтое, да? Это все в целом... Да, х это все желтые значения. Х, это х. Значит, дальше этот х с какими-то весами b0, обратите внимание, b0 plus означает, что b0 закрыта все веса, которые здесь на этими линиями, да? Плюс некий b0, тут тоже закрыта вся вибрация.
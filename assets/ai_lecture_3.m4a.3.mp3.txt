То есть это матричная запись, получается, и f0, да, это некая функция активации, применяется здесь, вот, и получается, что на выходе вот из этого, на выходе из этого мы получаем значение для этого слоя, для слоя 1. Дальше, соответственно, мы используем значение этого слоя, чтобы получить значение для этого слоя. И поэтому получается, что для того, чтобы посчитать значение этого слоя, мы значение вот этого слоя теперь умножаем на матрицу v1, и получается, что v1 закрыта все веса, которые здесь. И получается, как мы применяем f1, получаем значение здесь. Теперь дальше, вот чтобы получить значение для этого слоя, мы значение, которое получили здесь, умножаем на v2, то есть v2 закрыта все веса, и получаем, соответственно, значение здесь. То есть это получается здесь, в итоге мы получаем то, что здесь. Ну и, наконец, мы умножаем на v3 выходы из этого слоя, и получаем конечное значение. То есть получается, что v3 зашифровал все вот эти нейроны, которые здесь. И получается, в итоге, что если вот это все сделать, мы получаем выходные значения. Теперь обратите внимание, что это математическая запись. Я почему хотел подробно вам это рассказать, потому что получается, что, например, чтобы получить значение скрытого слоя 2, вам нужны значения скрытого слоя 1, которые, в свою очередь, получаются из значений вторных. То есть это как такой матрешка. Чем дальше идем, тем более запись вырастает. Но в корне этой записи лежит простая вещь. Вы просто используете значение предыдущего слоя, а, в свою очередь, чтобы посчитать этот слой, вы используете его значение еще более предыдущего слоя. В итоге получается, что, чтобы получить значение на выходе, у вас используются все вот эти связи, все слои и так далее. Ф получается функция активации? Да, везде это функция активации. Здесь это может быть на выходе. Я вам расскажу, что здесь может быть на выходе. То есть, на самом деле, если F0, F1, F2 это может быть Reul, то для F3 можно применить Softmax. А почему мы используем матрицу? То есть четыре значения, они дальше используются для расширения функции. Но вы можете просто взять какое-то конвертное значение и соответственно... То есть, например, на выходе самое простое, что можно взять, это смотреть самый большой вес. Ну, смотрите, то, о чем поговорили, это только вы смотрите значение, которое здесь, и наибольшее значение будет соответствовать индексу класса, который предсказывает нейронку. Но это вы на семинаре увидите. Короче говоря, да, это правильно то, что вы говорите. Поэтому, опять же, в плане, что на выходе получится, зависит от того, что у вас будет в качестве V3 и так далее. То есть, если у V3 размерность будет на выходе 4, то вы получите четыре значения. Правильно вообще? Ну да, да, правильно. Окей, и таким образом, давайте я опять разверну, вот это будет своя матрица весов. Ну, функция активации, это зависит от того, как вы зададите. Обычно в рамках одного слоя задают одну и ту же функцию активации. Но для разных слоев можно разные принципы. Для каждой значения? Для каждой перевалов подачи? Не, ну можно, но обычно так не делают. Почему так не делают? Потому что вам нужно будет делать обратное распространение ошибки. И чем сложнее вы это будете задавать, тем сложнее будет считать эти градиенты. То есть обычно все, кто пытается в этом плане не слишком переусердствовать. То есть по-хорошему на картинке между слоями надо нарисовать еще функцию активации? Ну да, тут и везде зашита функция активации. На выходе из каждого слоя вот здесь, грубо говоря, есть нелинейность этого слоя. То есть это матричная запись так называемого инференса. Что такое инференс? У вас есть на входе иксы, и вы их распространяете, чтобы получить выходные значения. То есть это распространение сигнала слева направо. Представьте, что вы уже нейронку обучили. Если на вход приходит икс, чтобы получить выходные значения, вам нужно совершить математические операции. Поэтому работа нейронок – это просто перемножение матриц между собой с математической точки зрения. А теперь давайте посмотрим, как их теперь обучать. Только вот мне, чтобы это вам рассказать. Да, вот сейчас получилось. Сейчас оно вернется. Если вопросы есть в чате, тоже задавайте. Сейчас я проверю, у нас все идет. Напишите там плюс всего нормального в чате. Да, все плюс. Спасибо. Сейчас проект отключится, и я продолжу. А теперь мы с вами будем говорить про обучение. Напомните, как мы уже умеем обучать модели машинного обучения. Какой мы метод с вами рассмотрели. То есть что нужно сделать, чтобы обучать модель машинного обучения. Камон, я в начале лекции что-то говорил. Ну, если вы подавали часа... Ну, веса, да. Вот как веса пропустить. Ну, нужны данные и результаты ответов. Так, и дальше что мы делаем? Вот у нас есть ответы, да, и есть выходы модели. Что с этим можно сказать? Считать ошибки. Дальше считаем ошибки. А как мы не без этого? Я это все почему? Потому что для нейронов остается все то же самое абсолютно. Уже знакомая вам формула. Мы для каждого веса. Представьте, у вас получается над каждой линией у вас есть свой какой-то вес. И вот для каждого веса вам нужно совершить такую операцию. И как вы видите, чтобы ее совершить, вам нужно посчитать производную. Причем много производной. И как вы можете понять, чем дальше у вас идет вглубь нейронки, тем больше надо сделать вычислений. Это логично. Потому что, как я вам говорил, смотрите, можно вернуться в эту запись. Чтобы посчитать производную по V0, вам нужно, видите, сколько здесь внешних оболочек. То есть получается 1, 2, 3. То есть вам нужно как минимум начать снаружи и внутрь вот так вот считать. И получается, что аналитически это делается не очень здорово. Особенно когда у вас глубокая нейронная сеть. Просто в лоб считать производную не очень здорово. Поэтому придумали такой лайфхак математический. Я про него говорил прямо на первой лекции. Он лежит в основе нейронных сетей. В частности, Джефф Дейхинтон, который получил в этом году за физику, он этот метод разработал. В чем заключается суть? Суть заключается в том, что мы сначала строим граф вычислений. Это просто некая иллюстрация абстрактная. Это не связано с этой картинкой. Это просто некая математическая абстракция, чтобы вам объяснить, в чем заключается метод. Представьте, что вы хотите обновить вот эти веса. При этом вычислительный граф. Представьте, что у вас какая-то абстрактная математическая функция. Чтобы получить выход, у вас есть две... Короче говоря, давайте я дальше сейчас... Что здесь нарисовать буду? Представьте, что получается у равно x1v1, и на это навешивается сигнал 1. В свою очередь сюда еще приходит сигнал отсюда, а сюда приходит сигнал и от x1, и от x2. И они все, короче, агрегируются здесь, и дальше мы получаем некий выход. Так вот, чтобы посчитать производное, можно воспользоваться лайфхаком. Мы вводим такие промежуточные перемены. Здесь, допустим, у1, у2. И чтобы посчитать производное по у1, вам нужно отследить от выхода. Теперь мы двигаемся справа налево. Нам нужно отследить, каким путем нам нужно пройти, чтобы добраться сюда. Мы видим, что мы идем справа налево. И вот по этому пути мы сюда набираемся. Теперь мы смотрим, какие у нас функции стоят на пути. Мы видим, что у нас возникла сигмойда, значит у нас будет производная dl по dсигмойда. Дальше мы берем производную сигмойда по у, потому что мы уже здесь, и нам сюда нужно. Теперь отсюда сюда. Это просто получается dy по dсигмойд. И, наконец, последняя производная получается dсигмойд по dy у1. И получается, чтобы посчитать вот эту производную, которая… Обратите внимание, наша задача – посчитать производную вот этой итоговой функции у1. И получается, что она, согласно методу взятия производных сложных функций, равна произведению всех этих производных. Получается, чтобы нам посчитать производную отсюда выходной функции по dсигмойд, нам нужно построить вычислительный граф, посмотреть путь распространения сигнала назад и взять все частные производные, которые у нас окажутся на пути, и перемножить их. На самом деле можно было бы просто напрямую считать вот эту штуку. Но я потом приведу пример. Можно взять какую-то функцию, например, ту же сигмойду, и брать напрямую производную. Но это практика показывает, что когда у вас сложные функции, это сделать гораздо сложнее, чем если вы разбиваете на вот такие функции. Потому что, как правило, такие производные по отдельности гораздо проще. И, собственно говоря, получается, что метод заключается в том, что вы берете…
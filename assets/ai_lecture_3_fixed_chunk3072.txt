Все, запись начала, отлично, теперь точно можно начинать. Так вот, быстро повторюсь, значит, мы сегодня с вами говорим про нейронные сети, наконец-то переходим к более сложным темам. Начнем с самых простых объектов архитектуры нейронных сетей, то есть с модели перцептрона. Из перцептронов мы будем делать с вами многослойные нейронные сети. Мы говорили про постановку задач машинного обучения с учителем. У нас есть данные X и ответы к ним Y, то есть это пример с квартирами: каждый объект — это некая квартира, у каждой квартиры есть признаки, и ответ — это цена. Нам нужно предсказать цену, то есть у нас есть X, у каждого X есть признаки X1, X2, X3 и так далее, и есть Y. Соответственно, задачи машинного обучения строятся таким образом, что нам нужно подобрать алгоритм A для получения отображения из X в Y. Понимаем, что невозможно сделать абсолютно точное отображение на всех данных, поэтому мы вводим функцию потерь, которая показывает, насколько близко мы приближаемся к ответам, то есть это функция ошибки. Задача сводится к минимизации этой функции ошибки, и мы изменяем параметры модели.

Мы успели поговорить про линейные модели. Формула для линейной модели выглядит таким образом — это матричная запись. Мы пользуемся матричными операциями, и вы увидите, почему именно ими. Функции для задачи регрессии это MSE и MAE. Мы говорили, как они выглядят, что они делают, и как это делается. Используем градиентный спуск. Чтобы посчитать градиентный спуск, нам нужно посчитать градиент. Линейная регрессия выглядит вот таким образом. Транспонирование зависит от того, как вы задаете матрицы X, Y и так далее. 

Говорили о задаче классификации, в частности, бинарной классификации. В классификации задача выглядит иначе: нам нужно выделить прямую, которая разделяет объекты на два класса. Для этого вводим сигмоидную функцию, на вход которой подаем линейную комбинацию. Предсказания Y будет этой функцией. У нас появляется промежуточная переменная Z, которая является линейной комбинацией. Класс функции для логистической регрессии выглядит таким образом. 

При этом предсказания принимают значения 0 или 1. Если 0, то это обнуляется. В всех случаях мы усредняем по всем объектам из датасета. Теперь будем строить с вами нейронные сети. 

На самом деле первые модели нейронных сетей появились еще в 20 веке, но они были в основном теоретическими. Не хватало вычислительных мощностей и данных для их обучения. В 90-х, несмотря на развитие компьютеров, существовали проблемы с хранением данных. Но начиная с нулевых, когда появились видеокарты, нейронные сети стали популярны, особенно для распознавания образов. AlexNet показала, что обучение нейронных сетей превосходит классические подходы компьютерного зрения по качеству.

С каждым новым набором данных качество нейронных сетей продолжает расти, в отличие от классических алгоритмов машинного обучения, которые приходят к плато. Мы видим, как компании вроде OpenAI используют большие модели и объемы данных для повышения качества. 

Нейронные сети стали возможны благодаря возможности хранить и обрабатывать большие объемы данных. Раньше применялись статистические подходы, но развитие видеокарт дало толчок к применению нейронных сетей. Стоит отметить, что проблемы линейных моделей возникают, если мы имеем нелинейные зависимости. 

Когда возникает нелинейность, линейная модель работает плохо. Например, зависимость цены от размера может выглядеть нереалистично для линейной модели. Здесь нужно придумать новые инструменты. В следующем семестре вы пройдете методы ближайших соседей, градиентный бустинг и случайный лес, которые способны приближать линейные зависимости лучше, чем линейные модели. Нейронные сети, если их хорошо обучить, могут решить эту задачу более эффективно. 

Существует теорема, которая говорит, что если у нас есть какая-то функция и задана точность приближения... эффекции, которую мы здесь обсуждаем, каждый нейрон получает на вход множество чисел, которые затем агрегируются и передаются дальше через функцию активации, производя одно выходное число. Если убрать функцию активации, то система будет замыкаться на линейных зависимостях и не сможет моделировать сложные, нелинейные связи, что и делает слои центральным компонентом нейронной сети.

Когда выходные сигналы формируются в каждом нейроне, важно понимать, что модель может передавать разные веса для каждой связи на выходном слое. Это значит, что обучение сети включает в себя оптимизацию этих весов, позволяя сети извлекать полезные зависимости из данных. Теперь же мы можем посмотреть на иллюстрацию, которая более детально демонстрирует процесс, о котором я говорил ранее. 

Если вы примените матричные операции к этим соединениям, вы сможете эффективно обрабатывать данные, что значительно ускоряет вычисления и улучшает производительность всей нейронной сети. Таким образом, методы матричной алгебры значимы для построения архитектуры нейронных сетей, что и сделает процесс обучения более эффективным. 

Мы продолжим с обсуждением конкретных функций активации, которые играют важную роль в нейронных сетях, и как они используются для решения задач с различными типами выходных данных. в конечном итоге мы достигаем нужных весов и вычисляем производные, следуя по этим промежуточным переменным. Это позволяет значительно упростить процесс вычисления градиентов и оптимизации весов. 

В общем, обучение нейронной сети сводится к тому, чтобы последовательно обновлять веса с помощью градиентного спуска, основанного на ошибках выхода. При этом важно, чтобы процесс обновления выполнялся эффективно, особенно в глубоких сетях, где количество параметров может достигать миллионов.

Следует отметить, что на практике используются различные оптимизационные алгоритмы, такие как Adam, RMSprop и другие, которые помогают быстрее сходиться к оптимальному решению, корректируя шаг обучения на основе момента и среднего значения градиентов.

Теперь давайте немного подробнее рассмотрим, как именно можно применить этот подход на практике. Вы можете задавать вопросы по ходу изложения, если что-то будет непонятно. тем более сложные структуры и представления формируются. Это явление называется иерархией признаков. В глубоких нейронных сетях мы можем наблюдать, как на более низких уровнях сети происходят базовые преобразования данных, а на высоких уровнях — более абстрактные интерпретации этих данных.

Теперь давайте рассмотрим, как эта иерархия признаков позволяет нейронным сетям эффективно справляться с различными задачами. Например, в задачах компьютерного зрения, на первом слое нейронной сети могут распознаваться простые формы, такие как края и углы. На следующих слоях обнаруживаются более сложные структуры, такие как текстуры и узоры. Наконец, на последних слоях нейронная сеть может распознавать целые объекты, такие как лица или машины.

Этот процесс получения высокоуровневых признаков иллюстрирует, почему глубокие нейронные сети часто дают лучшие результаты по сравнению с более простыми моделями. Обучение нейронной сети, несмотря на сложность, открывает новые горизонты в области машинного обучения и искусственного интеллекта благодаря возможности моделировать сложные зависимости в данных.

Таким образом, перед тем как начать обучение нейронной сети, важно правильно инициализировать веса и понимать, как эти веса влияют на обучение. Я надеюсь, что объяснение было достаточно подробным, и у вас возникли какие-то вопросы, которые мы можем обсудить. Тем у вас будет больше что-то осмысленное. Это если не интуитивным образом рассказывать. Есть целые работы, которые изучают, как слои нейронных сетей имеют смысл и пытаются выяснить, как слои создают нечто физическое и так далее. Особенно я видел работы, связанные с LLM, но суть в том, что чем больше у вас нейронная сеть, тем сложнее уже анализировать, потому что у нее множество слоев и параметров. Но некая интуиция в этом есть.

Что я еще хотел рассказать? В принципе, я объяснил, как устроены нейронные сети. Получается, что здание связано с архитектурой, но не только из-за этого. В целом, смотрите, я чуть подробнее расскажу. Эти люди, о которых я упомянул, один — это Джеффри Хинтон, а другой — Юша Бенжио, по-моему. Оба они стояли у истоков этой области. Один из них, как раз, предложил метод обратного распространения ошибки, а другим было вручено признание, за то что они разработали различные архитектуры, опираясь на физические инсайты. У них в голове крутилась некая физическая модель, которую они перевели в математику и таким образом получили новые результаты.

В итоге это стало существенным вкладом в нейронные сети, и именно поэтому был уделен акцент на физику, чтобы объяснить эти нейронные сети. Все эти модели, включая модель Больцмана и другие, были основаны на физических инсайтах. Однако нейронные сети стали активно применяться не только в физике, а и в разных областях.

Теперь, если вы захотите запрограммировать это своими руками, есть множество фреймворков, таких как PyTorch, которые автоматически строят вычислительные графы и рассчитывают производные. Если вы хотите выполнить это самостоятельно, вам нужно будет применить один лайфхак, который я сейчас расскажу. Лайфхак заключается в том, что стоит использовать как можно меньше циклов, поскольку это значительно замедляет процесс и усложняет код. Например, в NumPy существуют функции, позволяющие вам производить операции над целыми массивами без необходимости написания циклов.

Также, когда речь идет об операциях сложения, вычитания и умножения, не надо создавать циклы, так как NumPy это все обрабатывает автоматически. Например, если у вас есть массив и вам нужно применить функцию, такую как экспонента, вам не нужно писать цикл для каждого элемента.

Короче говоря, это просто некие советы по оптимизации, если вы захотите что-то реализовать самостоятельно. Мы как раз подходим к концу. В принципе, у меня ничего особенного больше не осталось. Я призываю вас посмотреть материалы, отдельно лекции.

В заключение, давайте обсудим, откуда появилась идея нейронной сети. Человеческий нейрон действительно состоит из ядра, в которое приходят сигналы от других нейронов, и эти сигналы обрабатываются и передаются дальше. Это, конечно, не прямая аналогия, так как нейробиологи до сих пор не полностью понимают, что происходит в ядре реального нейрона. Но это не более чем математическая абстракция. 

При этом важно понимать, что нелинейности в этой модели гораздо проще, чем то, что действительно происходит в биологических нейронах. В итоге, вы должны вынести из этой лекции несколько ключевых моментов: как устроен нейрон, как выглядит распространение сигнала и как работает механизм обратного распространения ошибки.

Если вам интересно подробно изучить эти формулы, вы можете найти записи лекций на YouTube. Для понимания мне лично очень понравилась лекция Андрю Нг, где он начинает с логистической регрессии и плавно переходит к нейронной сети, подробно объясняя каждую стадию. Так что, если вам это будет интересно, рекомендую.

Если есть вопросы, давайте обсудим. Если все понятно, дайте знать. Спасибо за внимание. До свидания.
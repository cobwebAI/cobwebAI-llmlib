Запись начата. Сегодня мы говорим про нейронные сети, и начнем с архитектуры нейронных сетей, а именно с модели перцептрона. Из перцептронов мы будем строить многослойные нейронные сети. Мы уже обсуждали задачи машинного обучения с учителем. У нас есть данные \(x\) и ответы \(y\). Пример с квартирами: каждая строка - это объект квартиры с признаками и ценой. Задача заключается в том, чтобы предсказать цену, то есть построить отображение из \(X\) в \(Y\) с помощью алгоритма \(A\).

Мы понимаем, что невозможно достичь абсолютно точного отображения, поэтому вводим функцию потерь (loss-функцию), которая показывает, насколько мы близки к ответам. Цель - минимизировать эту функцию, изменяя параметры модели. Мы также обсудили линейные модели, и формула для линейной модели выглядит как матричная запись, и вы увидите, почему используем матричные операции.

Для задачи регрессии используется среднеквадратичная ошибка (MSE), и для расчета градиентного спуска нам нужен градиент. Мы говорили о бинарной классификации, где необходимо выделить прямую, которая разделяет объекты на классы, вводя функцию активации. Функция для логистической регрессии принимает линейную комбинацию \(x\) и выдаёт значения от 0 до 1.

Проблема линейных моделей в том, что при наличии нелинейной зависимости их качество падает. Например, цена может зависеть от размера квартиры нелинейно. Нейронные сети, в отличие от линейных моделей, способны более точно аппроксимировать такие зависимости.

Нейронные сети стали популярны благодаря улучшению вычислительных возможностей и доступности большого объема данных. Первый значимый прорыв произошел с моделью AlexNet в области распознавания изображений, которая показала превосходство нейронных сетей над традиционными методами.

Сейчас нейронные сети могут обрабатывать табличные данные, аудио, изображения и текст. Качество нейронных сетей сильно зависит от объема данных, что делает их более эффективными по мере увеличения объема информации. В области глубокого обучения процесс выглядит так: у вас возникает идея, вы пишете код, проводите эксперименты и получаете обратную связь для улучшения модели. 

На следующем семестре вы изучите алгоритмы, такие как метод ближайших соседей и случайный лес, которые тоже могут справляться с нелинейными зависимостями, но нейронные сети, при правильном обучении, работают лучше в этом отношении. Существуют теоремы, подтверждающие, что сложные функции можно аппроксимировать с помощью нейронных сетей. Мы можем подобрать такую нейронную сеть, которая аппроксимирует любую нелинейную зависимость. У нас есть входной слой, промежуточные слои и выходной слой. Например, сигнал идет в красный нейрон, а каждый сигнал также идет в каждый из нейронов, поэтому возникает множество связей. Доказано, что любое абстрактное функцию можно аппроксимировать однослойной нейронной сетью.

Что касается структуры нейронной сети, входные сигналы — это наши данные, признаки, например, для квартир это размер, район и так далее. Цена является таргетом, то есть тем, что мы хотим предсказать. Входные сигналы идут с весами \(W_1, W_2\) и так далее, и сдвигом \(W_0\). Все это суммируется, и модель перцептрона включает в себя две части: суммирование сигналов и применение нелинейной функции активации.

Функции активации — это нелинейные функции. Мы уже упоминали логистическую регрессию с сигмоидой, но также есть гиперболический тангенс, который колеблется от -1 до 1, а также ReLU, которая равна нулю для отрицательных значений и равна \(x\) для положительных. Однако у ReLU есть проблемы с нулевыми значениями, и иногда применяют Leaky ReLU, которая использует другой наклон для отрицательных значений.

Если убрать функцию активации из перцептрона, останется просто линейная функция, и вся сложность нейронной сети исчезнет. Функция активации — ключевой момент для аппроксимации нелинейных зависимостей.

Мы также обсудили, что в отличие от логистической регрессии, где выходы суммируются в одно место, в многослойных нейронных сетях может быть множество слоев и нейронов. Например, в задаче классификации, как в ImageNet, может быть 1000 классов с соответствующими выходами.

Теперь давайте подробнее рассмотрим прямое распространение сигналов в нейронной сети. У нас есть слой \(h_{t-1}\) и выходной слой \(h_t\). Мы домножаем веса на значения из предыдущего слоя и добавляем некоторый сдвиг. В отличие от линейной модели, где просто домножаются веса на признаки, в нейронной сети у каждого нейрона будут свои связи с предыдущим слоем, и это отображается в виде матричного произведения.

Каждая связь имеет свой вес, который подбирается в процессе обучения, и это важно понимать для оптимизации нейронных сетей. агрегируются, и через функцию активации уже выходит одно число. Я когда-то задавался, почему нужны эти слои, почему нельзя сразу подавать вход в новую нейронную сеть? Мы понимаем, что функция активации нужна. Согласно теореме, с которой я начал, нужен хотя бы один слой, чтобы аппроксимировать нелинейную зависимость. Если убрать этот слой и сразу пустить сигнал, это будет линейная модель.

Каждый нейрон имеет свои связи с предыдущим слоем, и это отображается в виде матричного произведения. Входные сигналы могут быть представлены в матрице весов \(V1\) размером 4х3, где 4 нейрона принимают 3 входных сигнала. Таким образом, у каждого нейрона будет по три веса. В конце на выходе из каждого нейрона будет свой сдвиг, в отличие от линейной модели, где один сдвиг применяется ко всей модели.

Далее, чтобы получить значение выходного слоя, мы перемножаем значения предыдущего слоя на матрицу весов. В такой матричной записи каждое значение зависит от всех предыдущих. Например, чтобы рассчитать значение второго скрытого слоя, нужны значения первого слоя, которые получены из значений входного слоя.

Функции активации, такие как ReLU для скрытых слоев и Softmax для выходного слоя, определяют, как будет обрабатываться информация на каждом этапе. Важно, что выходные значения формируются на основе весов в матрице, и для каждого из них будет своя функция активации.

Теперь, когда мы понимаем, как работает прямое распространение в нейронной сети, давайте поговорим о том, как обучать модель машинного обучения. Основные шаги включают сбор данных, получение ответов и вычисление ошибки. Для минимизации ошибки нужно использовать производные весов.

По мере увеличения глубины нейронной сети количество вычислений возрастает, и для этого используется метод обратного распространения ошибки, который заключается в построении графа вычислений. Это позволяет эффективно укоренять все промежуточные переменные для вычисления производных и обновления весов. Чтобы вычислить производную по \( U1 \), нам нужно отслеживать путь от выхода. Мы движемся справа налево, проходя по пути, чтобы добраться сюда. На нашем пути возникает функция активации, например, сигмоида. Это означает, что у нас будет производная \( dL \) по \( d\sigma \). Затем мы берем производную сигмоида по \( Y \) — фактически, вычисляем \( dy \) по \( d\sigma_1 \). Наконец, находим последнюю производную \( d\sigma_1 \) по \( dU1 \).

Чтобы посчитать производную итоговой функции \( u1 \), мы используем метод взятия производных сложных функций. Получается, что для вычисления выходной функции по какой-то переменной нужно построить вычислительный граф, проследить путь распространения сигнала назад и взять все частные производные по пути, перемножив их. В действительности, можно было бы напрямую вычислить эту функцию, но это сложнее при наличии множества операций.

Метод, о котором мы говорим, называется цепным правилом или методом обратного распространения ошибки (backpropagation). В обучении нейронной сети сначала нужно посчитать loss-функцию, затем выполнить прямое распространение сигнала слева направо, получить выходной сигнал и, используя его, вычислить loss-функцию. После этого мы осуществляем обратное распространение, беря все производные на пути и перемножая их. Как только у нас появится градиент, мы применяем его в методе градиентного спуска.

Обратите внимание, что мы должны сохранять промежуточные значения при прямом распространении, так как они понадобятся при обратном. Мы одновременно храним выходные значения каждого слоя, что позволяет нам использовать их при расчёте градиентов.

Попробуйте нарисовать простую нейронную сеть с несколькими слоями и посчитать производные, используя метод цепного правила. Вы увидите, что некоторые клапаны будут слишком сложны, если не использовать промежуточные значения.

Важно, чтобы веса в сети были правильно инициализированы. Неправильная инициализация (например, все нули) может привести к симметричным градиентам и проблемам с обучением, таким как затухание градиента. Обычно, для избежания этого, веса инициализируются случайно, но в небольших пределах, чтобы избежать слишком больших значений.

Механизм обучения нейронной сети состоит в итеративном применении этих методов, пока функция ошибки не достигнет приемлемого значения. Глубокие представления в нейронных сетях означают, что в глубоких слоях создаются более простые элементы признаков. На выходах нейронов в более глубоких слоях формируются простые признаки, а чем ближе к выходу, тем они становятся более осмысленными. Это связано с процессом обучения.

Существует исследование о слоении нейронных сетей и их смысле. Особенно много работ связано с большими языковыми моделями (LLM). Чем больше нейронная сеть, тем сложнее ее анализировать из-за большого количества слоев и параметров. 

Основной метод обратного распространения ошибки был предложен Картоном и Хопкинсом, которые работали с физическими инсайтами, преобразуя их в математические модели, такие как модель Больцмана.

Если вы хотите реализовать нейронную сеть самостоятельно, учтите, что использование фреймворков, таких как PyTorch, значительно упрощает создание вычислительных графов и расчет производных. Чтобы оптимизировать код, старайтесь минимизировать использование циклов, так как это может сильно замедлять обработку. Например, в NumPy существуют функции, которые автоматически применяют операции к массивам без необходимости писать циклы.

В завершение, важно понять, что аналогия между нейронными сетями и человеческими нейронами не является прямой. Нейронные сети являются математической абстракцией и не полностью отражают процессы, происходящие в биологических нейронах. Основные моменты, которые нужно вынести из лекции: структура нейрона, распространение сигнала от входа к выходу и механизм обратного распространения ошибки. 

Кроме того, рекомендую посмотреть лекцию Эндрю Нг о логистической регрессии и нейронных сетях, чтобы лучше понять векторизацию и другие аспекты нейронного обучения. Егор далее расскажет, как это программировать и обучать. Если у вас есть вопросы, не стесняйтесь задавать их. Спасибо за внимание!